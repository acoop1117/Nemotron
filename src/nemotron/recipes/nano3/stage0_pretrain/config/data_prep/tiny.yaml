run:
  env:
    container: anyscale/ray:2.49.2-py312

# Tiny config for pretrain data preparation
#
# Usage:
#   nemotron nano3 data prep pretrain --config tiny

# Path to data blend JSON file
blend_path: ${oc.env:PWD}/src/nemotron/recipes/nano3/stage0_pretrain/config/data_prep/data_blend_raw_small.json

# Output directory for tokenized data
output_dir: ${oc.env:PWD}/../output/stage0_pretrain_tiny

# Number of output shards - smaller for tiny config
num_shards: 4

# Number of shards for validation split
valid_shards: 1

# Number of shards for test split
test_shards: 1

# Tokenizer configuration (nested)
tokenizer:
  model: nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-Base-BF16
  add_bos: false
  add_eos: true

# Default text field name in datasets
text_field: text

# Skip documents shorter than this (null = no limit)
min_doc_chars: null

# Truncate documents longer than this (null = no limit)
max_doc_tokens: null

# Limit rows per dataset for quick tests - small sample for tiny
sample: 100

# Force new run, ignoring cache
force: true

# Config name for artifact naming
config_name: tiny

# =============================================================================
# Stage Configs
# =============================================================================

# Planning stage config
plan:
  planner_cpus: 0.5

# Download stage config
download:
  batch_size: 1
  stage_cpus: 0.5
  hf_xet_high_performance: true
  hf_xet_concurrent_range_gets: 32
  max_retries: 3
  timeout_sec: 300
  max_concurrent_downloads: 8

# Tokenization stage config
tokenization:
  cpus_per_worker: 4

# =============================================================================
# Pipeline Config
# =============================================================================

# Pipeline observability settings
observability:
  pipeline_logging_interval_s: 5
  wandb_log_pipeline_stats: true
