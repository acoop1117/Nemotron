run:
  data: SFTDataArtifact-sft:latest
  model: pretrain:latest
  env:
    container: nvcr.io/nvidian/nemo:25.11-nano-v3.rc2

recipe:
  _target_: megatron.bridge.recipes.qwen.qwen3.qwen3_8b_finetune_config
  packed_sequence: true
  peft: null  # Disable LoRA, do full SFT

# Dataset config for pre-packed .npy files from data_prep
# train.py builds FinetuningDatasetConfig directly (not HFDatasetConfig) to skip HF download
dataset:
  dataset_root: ${art:data,path}
  seq_length: ${art:data,pack_size}
  packed_sequence_specs:
    packed_sequence_size: ${art:data,pack_size}
    packed_train_data_path: ${art:data,training_path}
    packed_val_data_path: ${art:data,validation_path}
    packed_metadata_path: ${art:data,metadata_path}

train:
  train_iters: 1700
  global_batch_size: 32

model:
  seq_length: ${art:data,pack_size}
  # Match pretrain parallelism settings for checkpoint compatibility
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1

# tokenizer:
#   tokenizer_type: HuggingFaceTokenizer
#   tokenizer_model: meta-llama/Llama-3.2-1B

scheduler:
  lr_warmup_iters: 32

logger:
  log_interval: 10
  wandb_project: ${run.wandb.project}   # Set from env.toml [wandb]
  wandb_entity: ${run.wandb.entity}     # Set from env.toml [wandb]

checkpoint:
  save: /nemo_run/sft
  save_interval: 20
  pretrained_checkpoint: ${art:model,path}
  finetune: true  # Skip loading optimizer state from pretrained checkpoint
  # ckpt_step: ${art:model,iteration}  # Optional: set to load specific iteration from artifact
