{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangGraph CLI Tool-Call Model Training Dataset Generator\n",
        "\n",
        "This notebook generates a synthetic dataset of natural language queries and their corresponding structured JSON CLI commands for training a model to convert user requests into LangGraph CLI tool calls.\n",
        "\n",
        "## Overview\n",
        "\n",
        "We use **NVIDIA's NeMo Data Designer** (open-source via Docker) to:\n",
        "1. Define a data schema with sampler columns for CLI parameters\n",
        "2. Generate natural language queries using Nemotron\n",
        "3. Create structured JSON output for each query\n",
        "4. Export the dataset for model fine-tuning\n",
        "\n",
        "### LangGraph CLI Commands Covered\n",
        "\n",
        "| Command | Description |\n",
        "|---------|-------------|\n",
        "| `langgraph new` | Create a new LangGraph project from a template |\n",
        "| `langgraph dev` | Run the development server with hot reload |\n",
        "| `langgraph up` | Launch the server in a Docker container |\n",
        "| `langgraph build` | Build the Docker image for deployment |\n",
        "| `langgraph dockerfile` | Generate a Dockerfile for the project |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Launch NeMo Data Designer via Docker Compose\n",
        "\n",
        "> ‚ö†Ô∏è **Before running this notebook**, follow the setup instructions in [README.md](./README.md) to start the NeMo Data Designer Docker service.\n",
        "\n",
        "The cells below are provided for convenience if you haven't started the service yet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Authenticate with NGC using your API key\n",
        "!echo $NGC_CLI_API_KEY | docker login nvcr.io -u '$oauthtoken' --password-stdin\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the NeMo Data Designer Docker Compose configuration\n",
        "!ngc registry resource download-version \"nvidia/nemo-microservices/nemo-data-designer-docker-compose:25.08\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start the Data Designer services (runs on port 8000)\n",
        "!cd nemo-data-designer-docker-compose_v25.08 && docker compose -f docker-compose.ea.yaml up -d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wait for services to be ready (optional health check)\n",
        "import time\n",
        "import requests\n",
        "\n",
        "print(\"Waiting for Data Designer service to be ready...\")\n",
        "max_retries = 30\n",
        "for i in range(max_retries):\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:8000/health\", timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            print(\"‚úì Data Designer service is ready!\")\n",
        "            break\n",
        "    except requests.exceptions.RequestException:\n",
        "        pass\n",
        "    time.sleep(10)\n",
        "    print(f\"  Retry {i+1}/{max_retries}...\")\n",
        "else:\n",
        "    print(\"‚ö† Service may not be ready. Proceeding anyway...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Install NeMo Data Designer SDK\n",
        "\n",
        "Install the SDK and connect to the local Data Designer service.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install \"nemo-microservices[data-designer]\" -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nemo_microservices.data_designer.essentials import (\n",
        "    DataDesignerConfigBuilder,\n",
        "    LLMTextColumnConfig,\n",
        "    SamplerColumnConfig,\n",
        "    SamplerType,\n",
        "    CategorySamplerParams,\n",
        "    BooleanSamplerParams,\n",
        "    UniformSamplerParams,\n",
        "    NeMoDataDesignerClient\n",
        ")\n",
        "\n",
        "# Connect to local Data Designer service (no API key needed for local)\n",
        "client = NeMoDataDesignerClient(base_url=\"http://localhost:8000/v1/nemo/dd\")\n",
        "print(\"‚úì Connected to NeMo Data Designer\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Define Schema for Synthetic Dataset\n",
        "\n",
        "We define a schema with:\n",
        "- **Sampler columns**: Randomly generated values for CLI parameters\n",
        "- **LLM text columns**: Natural language and structured JSON generated by Nemotron\n",
        "\n",
        "### Schema Overview\n",
        "\n",
        "| Column | Type | Description |\n",
        "|--------|------|-------------|\n",
        "| `command` | Category | One of: new, dev, up, build, dockerfile |\n",
        "| `template` | Category | Project template for 'new' command |\n",
        "| `include_path` | Boolean | Whether user specifies a custom path |\n",
        "| `port` | Uniform Int | Port number (3000-9000) |\n",
        "| `no_browser` | Boolean | Skip opening browser |\n",
        "| `watch` | Boolean | Watch for code changes |\n",
        "| `image_tag` | Category | Docker image tag |\n",
        "| `dockerfile_path` | Category | Custom Dockerfile path |\n",
        "| `input` | LLM Text | Natural language user query |\n",
        "| `output` | LLM Text | Structured JSON tool-call |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the configuration builder\n",
        "config_builder = DataDesignerConfigBuilder()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column 1: CLI command type (randomly choose one of the 5 LangGraph CLI commands)\n",
        "config_builder.add_column(\n",
        "    SamplerColumnConfig(\n",
        "        name=\"command\",\n",
        "        sampler_type=SamplerType.CATEGORY,\n",
        "        params=CategorySamplerParams(\n",
        "            values=[\"new\", \"dev\", \"up\", \"build\", \"dockerfile\"]\n",
        "        )\n",
        "    )\n",
        ")\n",
        "print(\"‚úì Added 'command' column\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column 2: Template name for 'new' command (choose from available templates)\n",
        "config_builder.add_column(\n",
        "    SamplerColumnConfig(\n",
        "        name=\"template\",\n",
        "        sampler_type=SamplerType.CATEGORY,\n",
        "        params=CategorySamplerParams(\n",
        "            values=[\n",
        "                \"basic\",\n",
        "                \"react-agent\",\n",
        "                \"memory-agent\",\n",
        "                \"retrieval-agent\",\n",
        "                \"data-enrichment\"\n",
        "            ]\n",
        "        )\n",
        "    )\n",
        ")\n",
        "print(\"‚úì Added 'template' column\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column 3: Whether user specifies a custom path (25% chance)\n",
        "config_builder.add_column(\n",
        "    SamplerColumnConfig(\n",
        "        name=\"include_path\",\n",
        "        sampler_type=SamplerType.CATEGORY,\n",
        "        params=CategorySamplerParams(\n",
        "            values=[True, False],\n",
        "            weights=[1, 3]  # 25% True, 75% False\n",
        "        )\n",
        "    )\n",
        ")\n",
        "print(\"‚úì Added 'include_path' column\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column 4: Port number (uniform distribution between 3000-9000)\n",
        "config_builder.add_column(\n",
        "    SamplerColumnConfig(\n",
        "        name=\"port\",\n",
        "        sampler_type=SamplerType.UNIFORM,\n",
        "        params=UniformSamplerParams(low=3000, high=9000),\n",
        "        convert_to=\"int\"\n",
        "    )\n",
        ")\n",
        "print(\"‚úì Added 'port' column\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column 5: No-browser flag (20% chance)\n",
        "config_builder.add_column(\n",
        "    SamplerColumnConfig(\n",
        "        name=\"no_browser\",\n",
        "        sampler_type=SamplerType.CATEGORY,\n",
        "        params=CategorySamplerParams(\n",
        "            values=[True, False],\n",
        "            weights=[1, 4]  # 20% True, 80% False\n",
        "        )\n",
        "    )\n",
        ")\n",
        "print(\"‚úì Added 'no_browser' column\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column 6: Watch flag for 'up' command (33% chance)\n",
        "config_builder.add_column(\n",
        "    SamplerColumnConfig(\n",
        "        name=\"watch\",\n",
        "        sampler_type=SamplerType.CATEGORY,\n",
        "        params=CategorySamplerParams(\n",
        "            values=[True, False],\n",
        "            weights=[1, 2]  # 33% True, 67% False\n",
        "        )\n",
        "    )\n",
        ")\n",
        "print(\"‚úì Added 'watch' column\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column 7: Docker image tag\n",
        "config_builder.add_column(\n",
        "    SamplerColumnConfig(\n",
        "        name=\"image_tag\",\n",
        "        sampler_type=SamplerType.CATEGORY,\n",
        "        params=CategorySamplerParams(\n",
        "            values=[\n",
        "                \"myapp:latest\",\n",
        "                \"latest\",\n",
        "                \"langgraph-app:v1\",\n",
        "                \"\"  # Empty = no custom tag\n",
        "            ]\n",
        "        )\n",
        "    )\n",
        ")\n",
        "print(\"‚úì Added 'image_tag' column\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column 8: Dockerfile path\n",
        "config_builder.add_column(\n",
        "    SamplerColumnConfig(\n",
        "        name=\"dockerfile_path\",\n",
        "        sampler_type=SamplerType.CATEGORY,\n",
        "        params=CategorySamplerParams(\n",
        "            values=[\n",
        "                \"Dockerfile\",\n",
        "                \"Dockerfile.custom\",\n",
        "                \"\"  # Empty = default path\n",
        "            ]\n",
        "        )\n",
        "    )\n",
        ")\n",
        "print(\"‚úì Added 'dockerfile_path' column\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLM-Generated Columns\n",
        "\n",
        "Now we add the columns that use Nemotron to generate:\n",
        "1. **Natural language input**: A user request in plain English\n",
        "2. **Structured JSON output**: The corresponding CLI tool-call\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column 9: Natural language query (generated by LLM using the sampled fields)\n",
        "input_prompt = \"\"\"You are an IT assistant. A user wants to use LangGraph CLI. \n",
        "Their goal: for command '{{ command }}'. \n",
        "{% if command == 'new' %}They want to create a new LangGraph project using the '{{ template }}' template\\\n",
        "{{ ' in directory \\\\'my_project\\\\'' if include_path else '' }}.\\\n",
        "{% elif command == 'dev' %}\\\n",
        "They want to run the dev server{% if port < 8000 %} on port {{ port }}{% endif %}\\\n",
        "{% if no_browser %} without opening a browser{% endif %}.\\\n",
        "{% elif command == 'up' %}\\\n",
        "They want to launch the server container{% if port >= 8000 %} on port {{ port }}{% endif %}\\\n",
        "{% if watch %} and have it watch for code changes{% endif %}.\\\n",
        "{% elif command == 'build' %}\\\n",
        "They need to build the Docker image{% if image_tag %} with tag '{{ image_tag }}'{% endif %}.\\\n",
        "{% elif command == 'dockerfile' %}\\\n",
        "They want to generate a Dockerfile{% if dockerfile_path and dockerfile_path != 'Dockerfile' %} at '{{ dockerfile_path }}'{% endif %}.\\\n",
        "{% endif %} \n",
        "Craft a concise request.\"\"\"\n",
        "\n",
        "config_builder.add_column(\n",
        "    LLMTextColumnConfig(\n",
        "        name=\"input\",  # User query text\n",
        "        model_alias=\"nemotron-nano-v2\",  # Nemotron 9B v2 model\n",
        "        prompt=input_prompt,\n",
        "        system_prompt=\"Respond with a one-sentence request, politely phrased, using any details provided.\"\n",
        "    )\n",
        ")\n",
        "print(\"‚úì Added 'input' column (natural language query)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column 10: Structured JSON output (LLM generates the CLI tool-call JSON)\n",
        "output_prompt = \"\"\"You are a CLI assistant. Convert the user request to a JSON tool-call object for LangGraph CLI. \n",
        "Only output valid JSON. Use keys: 'command' (string) and include any relevant options as additional keys.\n",
        "User wants to: {{ input }}\n",
        "JSON:\"\"\"\n",
        "\n",
        "config_builder.add_column(\n",
        "    LLMTextColumnConfig(\n",
        "        name=\"output\",  # Structured JSON representation\n",
        "        model_alias=\"nemotron-nano-v2\",\n",
        "        prompt=output_prompt,\n",
        "        system_prompt=\"You output JSON only, with correct keys for the CLI command.\"\n",
        "    )\n",
        ")\n",
        "print(\"‚úì Added 'output' column (structured JSON)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Generate Synthetic Data\n",
        "\n",
        "Now we generate the synthetic dataset using the Data Designer preview endpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic data (50 examples for preview)\n",
        "NUM_RECORDS = 50\n",
        "\n",
        "print(f\"Generating {NUM_RECORDS} synthetic examples...\")\n",
        "print(\"This may take a few minutes depending on your hardware.\\n\")\n",
        "\n",
        "preview = client.preview(config_builder, num_records=NUM_RECORDS)\n",
        "df = preview.dataset  # Pandas DataFrame of results\n",
        "\n",
        "print(f\"‚úì Generated {len(df)} examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample input/output pairs\n",
        "print(\"Sample Input/Output Pairs:\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for idx, row in df[['input', 'output']].head(5).iterrows():\n",
        "    print(f\"\\nüìù Input:  {row['input']}\")\n",
        "    print(f\"üíª Output: {row['output']}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show distribution of commands in the dataset\n",
        "print(\"\\nCommand Distribution:\")\n",
        "print(df['command'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview the full dataframe\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Export Dataset to JSONL\n",
        "\n",
        "Export the synthetic dataset in JSONL format for model fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export full dataset to JSONL\n",
        "output_file = \"langgraph_cli_synthetic.jsonl\"\n",
        "df.to_json(output_file, orient=\"records\", lines=True)\n",
        "print(f\"‚úì Saved {len(df)} examples to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export only input/output pairs (for training)\n",
        "training_file = \"langgraph_cli_training.jsonl\"\n",
        "df[['input', 'output']].to_json(training_file, orient=\"records\", lines=True)\n",
        "print(f\"‚úì Saved training pairs to {training_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview the exported file\n",
        "print(\"\\nPreview of exported JSONL:\\n\")\n",
        "!head -3 langgraph_cli_training.jsonl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Generate Larger Dataset (Optional)\n",
        "\n",
        "For production training, you'll want more examples. Adjust `NUM_RECORDS` as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to generate a larger dataset\n",
        "# NUM_RECORDS_LARGE = 500\n",
        "# \n",
        "# print(f\"Generating {NUM_RECORDS_LARGE} synthetic examples...\")\n",
        "# preview_large = client.preview(config_builder, num_records=NUM_RECORDS_LARGE)\n",
        "# df_large = preview_large.dataset\n",
        "# \n",
        "# df_large.to_json(\"langgraph_cli_synthetic_large.jsonl\", orient=\"records\", lines=True)\n",
        "# print(f\"‚úì Saved {len(df_large)} examples to langgraph_cli_synthetic_large.jsonl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup\n",
        "\n",
        "When you're done, stop the Data Designer services. See [README.md](./README.md) for cleanup instructions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop the Data Designer services (uncomment to run)\n",
        "# !cd nemo-data-designer-docker-compose_v25.08 && docker compose -f docker-compose.ea.yaml down\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. **Validate the data**: Review the generated examples for quality\n",
        "2. **Fine-tune a model**: Use the JSONL file to fine-tune a model (e.g., with NeMo Framework)\n",
        "3. **Integrate with your agent**: Use the trained model to power a LangGraph CLI assistant\n",
        "\n",
        "### Example Training Commands\n",
        "\n",
        "```bash\n",
        "# Using NeMo Framework for fine-tuning\n",
        "python -m nemo.collections.nlp.models.language_modeling.megatron_gpt_sft \\\n",
        "    --config-path=conf \\\n",
        "    --config-name=megatron_gpt_sft.yaml \\\n",
        "    model.data.train_ds.file_path=langgraph_cli_training.jsonl\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
