{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph CLI Synthetic Dataset Generation\n",
    "\n",
    "This notebook demonstrates how to use **NVIDIA NeMo Data Designer** to create a synthetic dataset for training an AI agent to translate natural language queries into structured CLI tool calls.\n",
    "\n",
    "## What is NeMo Data Designer?\n",
    "\n",
    "[NeMo Data Designer](https://github.com/NVIDIA-NeMo/DataDesigner) is a powerful synthetic data generation library that transforms data designs into high-quality datasets. It supports:\n",
    "\n",
    "- **Sampling-based columns**: Generate values from statistical distributions (uniform, categorical, etc.)\n",
    "- **LLM-based columns**: Use language models to generate realistic text or structured outputs\n",
    "- **Expression columns**: Compute values based on other columns using Python expressions\n",
    "- **Jinja templating**: Create dynamic prompts with conditional logic\n",
    "\n",
    "## Use Case: LangGraph CLI Agent\n",
    "\n",
    "We're generating training data for an agent that can interpret natural language requests like:\n",
    "> \"Create a new project using the react-agent template\"\n",
    "\n",
    "And convert them to structured tool calls:\n",
    "```json\n",
    "{\"command\": \"new\", \"template\": \"react-agent\", \"path\": null, ...}\n",
    "```\n",
    "\n",
    "This synthetic data will enable fine-tuning an LLM to perform accurate tool-calling for the LangGraph CLI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Provide NVIDIA API Key\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import getpass\n",
    "\n",
    "os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Data Designer\n",
    "\n",
    "Install and import the Data Designer library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_designer.essentials import (\n",
    "    DataDesigner,\n",
    "    DataDesignerConfigBuilder,\n",
    "    SamplerColumnConfig,\n",
    "    LLMTextColumnConfig,\n",
    "    LLMStructuredColumnConfig,\n",
    "    SamplerType,\n",
    "    CategorySamplerParams,\n",
    "    UniformSamplerParams,\n",
    "    ModelConfig,\n",
    "    ChatCompletionInferenceParams,\n",
    ")\n",
    "\n",
    "designer = DataDesigner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Design the Synthetic Data Schema\n",
    "\n",
    "Define the structure of our synthetic dataset using Data Designer's column types:\n",
    "\n",
    "- **Pydantic Model**: `CLIToolCall` schema for structured outputs\n",
    "- **Model Config**: LLM settings for data generation\n",
    "- **Sampler Columns**: Statistical distributions for seed values\n",
    "- **LLM Columns**: Generate natural language inputs and structured outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<meta charset=\"UTF-8\">\n",
       "<style>\n",
       "pre { line-height: 125%; }\n",
       "td.linenos .normal { color: #D8DEE9; background-color: #242933; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: #D8DEE9; background-color: #242933; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #242933; background-color: #D8DEE9; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #242933; background-color: #D8DEE9; padding-left: 5px; padding-right: 5px; }\n",
       ".code .hll { background-color: #3B4252 }\n",
       ".code { background: #2E3440; color: #D8DEE9 }\n",
       ".code .c { color: #616E87; font-style: italic } /* Comment */\n",
       ".code .err { color: #BF616A } /* Error */\n",
       ".code .esc { color: #D8DEE9 } /* Escape */\n",
       ".code .g { color: #D8DEE9 } /* Generic */\n",
       ".code .k { color: #81A1C1; font-weight: bold } /* Keyword */\n",
       ".code .l { color: #D8DEE9 } /* Literal */\n",
       ".code .n { color: #D8DEE9 } /* Name */\n",
       ".code .o { color: #81A1C1; font-weight: bold } /* Operator */\n",
       ".code .x { color: #D8DEE9 } /* Other */\n",
       ".code .p { color: #ECEFF4 } /* Punctuation */\n",
       ".code .ch { color: #616E87; font-style: italic } /* Comment.Hashbang */\n",
       ".code .cm { color: #616E87; font-style: italic } /* Comment.Multiline */\n",
       ".code .cp { color: #5E81AC; font-style: italic } /* Comment.Preproc */\n",
       ".code .cpf { color: #616E87; font-style: italic } /* Comment.PreprocFile */\n",
       ".code .c1 { color: #616E87; font-style: italic } /* Comment.Single */\n",
       ".code .cs { color: #616E87; font-style: italic } /* Comment.Special */\n",
       ".code .gd { color: #BF616A } /* Generic.Deleted */\n",
       ".code .ge { color: #D8DEE9; font-style: italic } /* Generic.Emph */\n",
       ".code .ges { color: #D8DEE9; font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".code .gr { color: #BF616A } /* Generic.Error */\n",
       ".code .gh { color: #88C0D0; font-weight: bold } /* Generic.Heading */\n",
       ".code .gi { color: #A3BE8C } /* Generic.Inserted */\n",
       ".code .go { color: #D8DEE9 } /* Generic.Output */\n",
       ".code .gp { color: #616E88; font-weight: bold } /* Generic.Prompt */\n",
       ".code .gs { color: #D8DEE9; font-weight: bold } /* Generic.Strong */\n",
       ".code .gu { color: #88C0D0; font-weight: bold } /* Generic.Subheading */\n",
       ".code .gt { color: #BF616A } /* Generic.Traceback */\n",
       ".code .kc { color: #81A1C1; font-weight: bold } /* Keyword.Constant */\n",
       ".code .kd { color: #81A1C1; font-weight: bold } /* Keyword.Declaration */\n",
       ".code .kn { color: #81A1C1; font-weight: bold } /* Keyword.Namespace */\n",
       ".code .kp { color: #81A1C1 } /* Keyword.Pseudo */\n",
       ".code .kr { color: #81A1C1; font-weight: bold } /* Keyword.Reserved */\n",
       ".code .kt { color: #81A1C1 } /* Keyword.Type */\n",
       ".code .ld { color: #D8DEE9 } /* Literal.Date */\n",
       ".code .m { color: #B48EAD } /* Literal.Number */\n",
       ".code .s { color: #A3BE8C } /* Literal.String */\n",
       ".code .na { color: #8FBCBB } /* Name.Attribute */\n",
       ".code .nb { color: #81A1C1 } /* Name.Builtin */\n",
       ".code .nc { color: #8FBCBB } /* Name.Class */\n",
       ".code .no { color: #8FBCBB } /* Name.Constant */\n",
       ".code .nd { color: #D08770 } /* Name.Decorator */\n",
       ".code .ni { color: #D08770 } /* Name.Entity */\n",
       ".code .ne { color: #BF616A } /* Name.Exception */\n",
       ".code .nf { color: #88C0D0 } /* Name.Function */\n",
       ".code .nl { color: #D8DEE9 } /* Name.Label */\n",
       ".code .nn { color: #8FBCBB } /* Name.Namespace */\n",
       ".code .nx { color: #D8DEE9 } /* Name.Other */\n",
       ".code .py { color: #D8DEE9 } /* Name.Property */\n",
       ".code .nt { color: #81A1C1 } /* Name.Tag */\n",
       ".code .nv { color: #D8DEE9 } /* Name.Variable */\n",
       ".code .ow { color: #81A1C1; font-weight: bold } /* Operator.Word */\n",
       ".code .pm { color: #ECEFF4 } /* Punctuation.Marker */\n",
       ".code .w { color: #D8DEE9 } /* Text.Whitespace */\n",
       ".code .mb { color: #B48EAD } /* Literal.Number.Bin */\n",
       ".code .mf { color: #B48EAD } /* Literal.Number.Float */\n",
       ".code .mh { color: #B48EAD } /* Literal.Number.Hex */\n",
       ".code .mi { color: #B48EAD } /* Literal.Number.Integer */\n",
       ".code .mo { color: #B48EAD } /* Literal.Number.Oct */\n",
       ".code .sa { color: #A3BE8C } /* Literal.String.Affix */\n",
       ".code .sb { color: #A3BE8C } /* Literal.String.Backtick */\n",
       ".code .sc { color: #A3BE8C } /* Literal.String.Char */\n",
       ".code .dl { color: #A3BE8C } /* Literal.String.Delimiter */\n",
       ".code .sd { color: #616E87 } /* Literal.String.Doc */\n",
       ".code .s2 { color: #A3BE8C } /* Literal.String.Double */\n",
       ".code .se { color: #EBCB8B } /* Literal.String.Escape */\n",
       ".code .sh { color: #A3BE8C } /* Literal.String.Heredoc */\n",
       ".code .si { color: #A3BE8C } /* Literal.String.Interpol */\n",
       ".code .sx { color: #A3BE8C } /* Literal.String.Other */\n",
       ".code .sr { color: #EBCB8B } /* Literal.String.Regex */\n",
       ".code .s1 { color: #A3BE8C } /* Literal.String.Single */\n",
       ".code .ss { color: #A3BE8C } /* Literal.String.Symbol */\n",
       ".code .bp { color: #81A1C1 } /* Name.Builtin.Pseudo */\n",
       ".code .fm { color: #88C0D0 } /* Name.Function.Magic */\n",
       ".code .vc { color: #D8DEE9 } /* Name.Variable.Class */\n",
       ".code .vg { color: #D8DEE9 } /* Name.Variable.Global */\n",
       ".code .vi { color: #D8DEE9 } /* Name.Variable.Instance */\n",
       ".code .vm { color: #D8DEE9 } /* Name.Variable.Magic */\n",
       ".code .il { color: #B48EAD } /* Literal.Number.Integer.Long */\n",
       "\n",
       ".code {\n",
       "  padding: 4px;\n",
       "  border: 1px solid grey;\n",
       "  border-radius: 4px;\n",
       "  max-width: 1000px;\n",
       "  width: 100%;\n",
       "  display: inline-block;\n",
       "  box-sizing: border-box;\n",
       "  text-align: left;\n",
       "  vertical-align: top;\n",
       "  line-height: normal;\n",
       "  overflow-x: auto;\n",
       "}\n",
       "\n",
       ".code pre {\n",
       "  white-space: pre-wrap;       /* CSS 3 */\n",
       "  white-space: -moz-pre-wrap;  /* Mozilla, since 1999 */\n",
       "  white-space: -pre-wrap;      /* Opera 4-6 */\n",
       "  white-space: -o-pre-wrap;    /* Opera 7 */\n",
       "  word-wrap: break-word;\n",
       "  overflow-wrap: break-word;\n",
       "  margin: 0;\n",
       "}\n",
       "</style>\n",
       "<div class=\"code\"><pre><span></span><span class=\"n\">DataDesignerConfigBuilder</span><span class=\"p\">(</span>\n",
       "    <span class=\"n\">sampler_columns</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n",
       "        <span class=\"s2\">&quot;command&quot;</span><span class=\"p\">,</span>\n",
       "        <span class=\"s2\">&quot;template&quot;</span><span class=\"p\">,</span>\n",
       "        <span class=\"s2\">&quot;include_path&quot;</span><span class=\"p\">,</span>\n",
       "        <span class=\"s2\">&quot;port&quot;</span><span class=\"p\">,</span>\n",
       "        <span class=\"s2\">&quot;no_browser&quot;</span><span class=\"p\">,</span>\n",
       "        <span class=\"s2\">&quot;watch&quot;</span><span class=\"p\">,</span>\n",
       "        <span class=\"s2\">&quot;image_tag&quot;</span><span class=\"p\">,</span>\n",
       "        <span class=\"s2\">&quot;dockerfile_path&quot;</span>\n",
       "    <span class=\"p\">]</span>\n",
       "    <span class=\"n\">llm_text_columns</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s1\">&#39;input&#39;</span><span class=\"p\">]</span>\n",
       "    <span class=\"n\">llm_structured_columns</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s1\">&#39;output&#39;</span><span class=\"p\">]</span>\n",
       "<span class=\"p\">)</span>\n",
       "</pre></div>\n",
       "\n"
      ],
      "text/plain": [
       "DataDesignerConfigBuilder(\n",
       "    sampler_columns: [\n",
       "        \"command\",\n",
       "        \"template\",\n",
       "        \"include_path\",\n",
       "        \"port\",\n",
       "        \"no_browser\",\n",
       "        \"watch\",\n",
       "        \"image_tag\",\n",
       "        \"dockerfile_path\"\n",
       "    ]\n",
       "    llm_text_columns: ['input']\n",
       "    llm_structured_columns: ['output']\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "class CLIToolCall(BaseModel):\n",
    "    command: str = Field(..., description=\"CLI command: new, dev, up, build, or dockerfile\")\n",
    "    template: Optional[str] = Field(None, description=\"Template name for 'new' command\")\n",
    "    path: Optional[str] = Field(None, description=\"Project path for 'new' command\")\n",
    "    port: Optional[int] = Field(None, description=\"Port for 'dev' or 'up' command\")\n",
    "    no_browser: Optional[bool] = Field(None, description=\"Skip browser for 'dev' command\")\n",
    "    watch: Optional[bool] = Field(None, description=\"Watch mode for 'up' command\")\n",
    "    tag: Optional[str] = Field(None, description=\"Image tag for 'build' command\")\n",
    "    output_path: Optional[str] = Field(None, description=\"Output path for 'dockerfile' command\")\n",
    "\n",
    "# Model config\n",
    "model_configs = [\n",
    "    ModelConfig(\n",
    "        alias=\"command-generator\",\n",
    "        provider=\"nvidia\",\n",
    "        model=\"nvidia/nemotron-3-nano-30b-a3b\",\n",
    "        inference_parameters=ChatCompletionInferenceParams(\n",
    "            temperature=1.0,\n",
    "            top_p=1.0,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "config_builder = DataDesignerConfigBuilder(model_configs=model_configs)\n",
    "\n",
    "# Sampler columns\n",
    "config_builder.add_column(\n",
    "    SamplerColumnConfig(\n",
    "        name=\"command\",\n",
    "        sampler_type=SamplerType.CATEGORY,\n",
    "        params=CategorySamplerParams(values=[\"new\", \"dev\", \"up\", \"build\", \"dockerfile\"])\n",
    "    )\n",
    ")\n",
    "\n",
    "config_builder.add_column(\n",
    "    SamplerColumnConfig(\n",
    "        name=\"template\",\n",
    "        sampler_type=SamplerType.CATEGORY,\n",
    "        params=CategorySamplerParams(values=[\"basic\", \"react-agent\", \"memory-agent\", \"retrieval-agent\", \"data-enrichment\"])\n",
    "    )\n",
    ")\n",
    "\n",
    "config_builder.add_column(\n",
    "    SamplerColumnConfig(\n",
    "        name=\"include_path\",\n",
    "        sampler_type=SamplerType.CATEGORY,\n",
    "        params=CategorySamplerParams(values=[True, False], weights=[1, 3])\n",
    "    )\n",
    ")\n",
    "\n",
    "config_builder.add_column(\n",
    "    SamplerColumnConfig(\n",
    "        name=\"port\",\n",
    "        sampler_type=SamplerType.UNIFORM,\n",
    "        params=UniformSamplerParams(low=3000, high=9000),\n",
    "        convert_to=\"int\"\n",
    "    )\n",
    ")\n",
    "\n",
    "config_builder.add_column(\n",
    "    SamplerColumnConfig(\n",
    "        name=\"no_browser\",\n",
    "        sampler_type=SamplerType.CATEGORY,\n",
    "        params=CategorySamplerParams(values=[True, False], weights=[1, 4])\n",
    "    )\n",
    ")\n",
    "\n",
    "config_builder.add_column(\n",
    "    SamplerColumnConfig(\n",
    "        name=\"watch\",\n",
    "        sampler_type=SamplerType.CATEGORY,\n",
    "        params=CategorySamplerParams(values=[True, False], weights=[1, 2])\n",
    "    )\n",
    ")\n",
    "\n",
    "config_builder.add_column(\n",
    "    SamplerColumnConfig(\n",
    "        name=\"image_tag\",\n",
    "        sampler_type=SamplerType.CATEGORY,\n",
    "        params=CategorySamplerParams(values=[\"myapp:latest\", \"latest\", \"langgraph-app:v1\"])\n",
    "    )\n",
    ")\n",
    "\n",
    "config_builder.add_column(\n",
    "    SamplerColumnConfig(\n",
    "        name=\"dockerfile_path\",\n",
    "        sampler_type=SamplerType.CATEGORY,\n",
    "        params=CategorySamplerParams(values=[\"Dockerfile\", \"Dockerfile.custom\", \"docker/Dockerfile\"])\n",
    "    )\n",
    ")\n",
    "\n",
    "# Input column - generates natural language requests\n",
    "config_builder.add_column(\n",
    "    LLMTextColumnConfig(\n",
    "        name=\"input\",\n",
    "        model_alias=\"command-generator\",\n",
    "        prompt=(\n",
    "            \"Generate a natural user request for the LangGraph CLI.\\n\\n\"\n",
    "            \"Command: {{ command }}\\n\\n\"\n",
    "            \"{% if command == 'new' %}\"\n",
    "            \"The user wants to create a new project with the '{{ template }}' template.\"\n",
    "            \"{% if include_path %} They want it in a custom directory.{% endif %}\"\n",
    "            \"{% elif command == 'dev' %}\"\n",
    "            \"The user wants to start the dev server on port {{ port }}.\"\n",
    "            \"{% if no_browser %} They don't want to auto-open a browser.{% endif %}\"\n",
    "            \"{% elif command == 'up' %}\"\n",
    "            \"The user wants to launch the server container on port {{ port }}.\"\n",
    "            \"{% if watch %} They want to watch for code changes.{% endif %}\"\n",
    "            \"{% elif command == 'build' %}\"\n",
    "            \"The user wants to build a Docker image with tag '{{ image_tag }}'.\"\n",
    "            \"{% elif command == 'dockerfile' %}\"\n",
    "            \"The user wants to generate a Dockerfile at '{{ dockerfile_path }}'.\"\n",
    "            \"{% endif %}\\n\\n\"\n",
    "            \"Write one natural, conversational sentence.\"\n",
    "        ),\n",
    "        system_prompt=\"Output only a single sentence. No explanation.\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Output column - generates structured CLI tool calls\n",
    "config_builder.add_column(\n",
    "    LLMStructuredColumnConfig(\n",
    "        name=\"output\",\n",
    "        model_alias=\"command-generator\",\n",
    "        prompt=(\n",
    "            \"Convert this user request to a LangGraph CLI tool-call.\\n\\n\"\n",
    "            \"Command type: {{ command }}\\n\"\n",
    "            \"User request: {{ input }}\\n\\n\"\n",
    "            \"{% if command == 'new' %}\"\n",
    "            \"Set: template, and path if specified.\"\n",
    "            \"{% elif command == 'dev' %}\"\n",
    "            \"Set: port, and no_browser if specified.\"\n",
    "            \"{% elif command == 'up' %}\"\n",
    "            \"Set: port, and watch if specified.\"\n",
    "            \"{% elif command == 'build' %}\"\n",
    "            \"Set: tag.\"\n",
    "            \"{% elif command == 'dockerfile' %}\"\n",
    "            \"Set: output_path.\"\n",
    "            \"{% endif %}\\n\\n\"\n",
    "            \"Only set fields relevant to the command. Leave others as null.\"\n",
    "        ),\n",
    "        system_prompt=\"Output ONLY the JSON object. No preamble, no quotes, no meta-commentary.\",\n",
    "        output_format=CLIToolCall,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Preview Data Generation\n",
    "\n",
    "Before generating a large dataset, we use the preview feature to validate our configuration and inspect sample outputs. This follows the recommended workflow:\n",
    "\n",
    "1. **Design phase** ‚Üí Define columns and prompts\n",
    "2. **Preview** ‚Üí Generate small batches to validate quality\n",
    "3. **Iterate** ‚Üí Refine prompts and constraints\n",
    "4. **Batch generation** ‚Üí Create full dataset\n",
    "\n",
    "The preview runs the full pipeline on a small sample (5 records here), returning a Pandas DataFrame with all generated columns.\n",
    "\n",
    "This lets us verify that:\n",
    "- Natural language inputs sound realistic\n",
    "- Structured outputs conform to our Pydantic schema\n",
    "- The command-to-input-to-output pipeline produces coherent training pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13:12:30] [INFO] üì∫ Preview generation in progress\n",
      "[13:12:32] [INFO] ‚úÖ Validation passed\n",
      "[13:12:32] [INFO] ‚õìÔ∏è Sorting column configs into a Directed Acyclic Graph\n",
      "[13:12:32] [INFO] ü©∫ Running health checks for models...\n",
      "[13:12:32] [INFO]   |-- üëÄ Checking 'nvidia/nemotron-3-nano-30b-a3b' in provider named 'nvidia' for model alias 'command-generator'...\n",
      "[13:12:33] [INFO]   |-- ‚úÖ Passed!\n",
      "[13:12:33] [INFO] üé≤ Preparing samplers to generate 5 records across 8 columns\n",
      "[13:12:34] [INFO] üìù llm-text model config for column 'input'\n",
      "[13:12:34] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n",
      "[13:12:34] [INFO]   |-- model alias: 'command-generator'\n",
      "[13:12:34] [INFO]   |-- model provider: 'nvidia'\n",
      "[13:12:34] [INFO]   |-- inference parameters: generation_type=chat-completion, max_parallel_requests=4, temperature=1.00, top_p=1.00, max_tokens=1000\n",
      "[13:12:34] [INFO] üêô Processing llm-text column 'input' with 4 concurrent workers\n",
      "[13:12:36] [INFO] üóÇÔ∏è llm-structured model config for column 'output'\n",
      "[13:12:36] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n",
      "[13:12:36] [INFO]   |-- model alias: 'command-generator'\n",
      "[13:12:36] [INFO]   |-- model provider: 'nvidia'\n",
      "[13:12:36] [INFO]   |-- inference parameters: generation_type=chat-completion, max_parallel_requests=4, temperature=1.00, top_p=1.00, max_tokens=1000\n",
      "[13:12:36] [INFO] üêô Processing llm-structured column 'output' with 4 concurrent workers\n",
      "[13:12:39] [INFO] üìä Model usage summary:\n",
      "{\n",
      "    \"nvidia/nemotron-3-nano-30b-a3b\": {\n",
      "        \"token_usage\": {\n",
      "            \"input_tokens\": 2964,\n",
      "            \"output_tokens\": 1837,\n",
      "            \"total_tokens\": 4801\n",
      "        },\n",
      "        \"request_usage\": {\n",
      "            \"successful_requests\": 10,\n",
      "            \"failed_requests\": 0,\n",
      "            \"total_requests\": 10\n",
      "        },\n",
      "        \"tokens_per_second\": 808,\n",
      "        \"requests_per_minute\": 101\n",
      "    }\n",
      "}\n",
      "[13:12:39] [INFO] üìê Measuring dataset column statistics:\n",
      "[13:12:39] [INFO]   |-- üé≤ column: 'command'\n",
      "[13:12:39] [INFO]   |-- üé≤ column: 'template'\n",
      "[13:12:39] [INFO]   |-- üé≤ column: 'include_path'\n",
      "[13:12:39] [INFO]   |-- üé≤ column: 'port'\n",
      "[13:12:39] [INFO]   |-- üé≤ column: 'no_browser'\n",
      "[13:12:39] [INFO]   |-- üé≤ column: 'watch'\n",
      "[13:12:39] [INFO]   |-- üé≤ column: 'image_tag'\n",
      "[13:12:39] [INFO]   |-- üé≤ column: 'dockerfile_path'\n",
      "[13:12:39] [INFO]   |-- üìù column: 'input'\n",
      "[13:12:39] [INFO]   |-- üóÇÔ∏è column: 'output'\n",
      "[13:12:39] [INFO] ‚òÄÔ∏è Preview complete!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               input  \\\n",
      "0  Run the LangGraph server inside the container ...   \n",
      "1  Could you start the server container on port 8...   \n",
      "2  Could you spin up the dev server on port‚ÄØ6446 ...   \n",
      "3  Could you build the Docker image and tag it as...   \n",
      "4  Could you start the server container and expos...   \n",
      "\n",
      "                                              output  \n",
      "0  {'command': 'up', 'template': None, 'path': No...  \n",
      "1  {'command': 'up', 'template': None, 'path': No...  \n",
      "2  {'command': 'dev', 'port': 6446, 'no_browser':...  \n",
      "3  {'command': 'build', 'template': None, 'path':...  \n",
      "4  {'command': 'up', 'template': None, 'path': No...  \n"
     ]
    }
   ],
   "source": [
    "# Preview with 5 records to validate the configuration\n",
    "preview_result = designer.preview(config_builder=config_builder, num_records=5)\n",
    "preview_df = preview_result.dataset\n",
    "print(preview_df[['input', 'output']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Full Dataset\n",
    "\n",
    "Once we're satisfied with the preview results, we generate the full dataset. The `designer.generate()` method processes data in batches with parallel LLM requests for efficiency.\n",
    "\n",
    "> NOTE: We use the following to supress warnings from Data Designer and build.nvidia.com responses having a cosmetic mistmatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13:13:21] [INFO] üé® Creating Data Designer dataset\n",
      "[13:13:21] [INFO] ‚úÖ Validation passed\n",
      "[13:13:21] [INFO] ‚õìÔ∏è Sorting column configs into a Directed Acyclic Graph\n",
      "[13:13:21] [INFO] ü©∫ Running health checks for models...\n",
      "[13:13:21] [INFO]   |-- üëÄ Checking 'nvidia/nemotron-3-nano-30b-a3b' in provider named 'nvidia' for model alias 'command-generator'...\n",
      "[13:13:23] [INFO]   |-- ‚úÖ Passed!\n",
      "[13:13:23] [INFO] ‚è≥ Processing batch 1 of 1\n",
      "[13:13:23] [INFO] üé≤ Preparing samplers to generate 250 records across 8 columns\n",
      "[13:13:23] [INFO] üìù llm-text model config for column 'input'\n",
      "[13:13:23] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n",
      "[13:13:23] [INFO]   |-- model alias: 'command-generator'\n",
      "[13:13:23] [INFO]   |-- model provider: 'nvidia'\n",
      "[13:13:23] [INFO]   |-- inference parameters: generation_type=chat-completion, max_parallel_requests=4, temperature=1.00, top_p=1.00, max_tokens=1000\n",
      "[13:13:23] [INFO] üêô Processing llm-text column 'input' with 4 concurrent workers\n",
      "[13:14:27] [INFO] üóÇÔ∏è llm-structured model config for column 'output'\n",
      "[13:14:27] [INFO]   |-- model: 'nvidia/nemotron-3-nano-30b-a3b'\n",
      "[13:14:27] [INFO]   |-- model alias: 'command-generator'\n",
      "[13:14:27] [INFO]   |-- model provider: 'nvidia'\n",
      "[13:14:27] [INFO]   |-- inference parameters: generation_type=chat-completion, max_parallel_requests=4, temperature=1.00, top_p=1.00, max_tokens=1000\n",
      "[13:14:27] [INFO] üêô Processing llm-structured column 'output' with 4 concurrent workers\n",
      "[13:16:21] [INFO] üìä Model usage summary:\n",
      "{\n",
      "    \"nvidia/nemotron-3-nano-30b-a3b\": {\n",
      "        \"token_usage\": {\n",
      "            \"input_tokens\": 146322,\n",
      "            \"output_tokens\": 96808,\n",
      "            \"total_tokens\": 243130\n",
      "        },\n",
      "        \"request_usage\": {\n",
      "            \"successful_requests\": 500,\n",
      "            \"failed_requests\": 0,\n",
      "            \"total_requests\": 500\n",
      "        },\n",
      "        \"tokens_per_second\": 1391,\n",
      "        \"requests_per_minute\": 171\n",
      "    }\n",
      "}\n",
      "[13:16:21] [INFO] üìê Measuring dataset column statistics:\n",
      "[13:16:21] [INFO]   |-- üé≤ column: 'command'\n",
      "[13:16:21] [INFO]   |-- üé≤ column: 'template'\n",
      "[13:16:21] [INFO]   |-- üé≤ column: 'include_path'\n",
      "[13:16:21] [INFO]   |-- üé≤ column: 'port'\n",
      "[13:16:21] [INFO]   |-- üé≤ column: 'no_browser'\n",
      "[13:16:21] [INFO]   |-- üé≤ column: 'watch'\n",
      "[13:16:21] [INFO]   |-- üé≤ column: 'image_tag'\n",
      "[13:16:21] [INFO]   |-- üé≤ column: 'dockerfile_path'\n",
      "[13:16:21] [INFO]   |-- üìù column: 'input'\n",
      "[13:16:22] [INFO]   |-- üóÇÔ∏è column: 'output'\n"
     ]
    }
   ],
   "source": [
    "# Generate 250 synthetic training examples\n",
    "generate_result = designer.create(config_builder=config_builder, num_records=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 250 records\n"
     ]
    }
   ],
   "source": [
    "dataset_df = generate_result.load_dataset()\n",
    "print(f\"Generated {len(dataset_df)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save Dataset for Training\n",
    "\n",
    "Export the generated dataset to JSONL format for use with the GRPO training pipeline. We split into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 225 training and 25 validation examples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Split into train/val (90/10)\n",
    "train_df, val_df = train_test_split(dataset_df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"data/langgraph_cli\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save as JSONL\n",
    "def save_jsonl(df, path):\n",
    "    with open(path, 'w') as f:\n",
    "        for _, row in df.iterrows():\n",
    "            record = {\"input\": row[\"input\"], \"output\": row[\"output\"]}\n",
    "            f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "save_jsonl(train_df, output_dir / \"train.jsonl\")\n",
    "save_jsonl(val_df, output_dir / \"val.jsonl\")\n",
    "\n",
    "print(f\"Saved {len(train_df)} training and {len(val_df)} validation examples\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
