{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Running Nemotron-49B-v1.5 with Hugging Face Transformers on NVIDIA GPUs\n",
        "\n",
        "This notebook provides a comprehensive guide on how to run the `Nemotron-49B-v1.5` model using Hugging Face Transformers library for direct inference and experimentation.\n",
        "\n",
        "This notebook will cover:\n",
        "- Loading the model and tokenizer with optimized configurations.\n",
        "- Demonstrating the model's reasoning modes (`think` vs `no_think`).\n",
        "- Basic chat completions and streaming responses.\n",
        "- Batch processing for multiple prompts.\n",
        "\n",
        "#### Launch on NVIDIA Brev\n",
        "You can simplify the environment setup by using [NVIDIA Brev](https://developer.nvidia.com/brev). Click the button below to launch this project on a Brev instance with the necessary dependencies pre-configured.\n",
        "\n",
        "Once deployed, click on the \"Open Notebook\" button to get started with this guide.\n",
        "\n",
        "[![Launch on Brev](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-32vt7HcQjCUpafGyquLZwJdIm8F)\n",
        "\n",
        "- Model card: [nvidia/Llama-3.3-Nemotron-Super-49B-v1.5](https://huggingface.co/nvidia/Llama-3.3-Nemotron-Super-49B-v1.5)\n",
        "- Transformers Docs: [https://huggingface.co/docs/transformers/](https://huggingface.co/docs/transformers/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "- [Prerequisites](#Prerequisites)\n",
        "- [Setup](#Setup)\n",
        "- [Load Model and Tokenizer](#Load-Model-and-Tokenizer)\n",
        "- [Helper Functions](#Helper-Functions)\n",
        "- [Showcasing Reasoning Modes: `think` vs. `no_think`](#Showcasing-Reasoning-Modes:-`think`-vs.-`no_think`)\n",
        "- [Simple Chat Completion](#Simple-Chat-Completion)\n",
        "- [Streaming](#Streaming)\n",
        "- [Batching](#Batching)\n",
        "- [Resource Notes](#Resource-Notes)\n",
        "- [Conclusion](#Conclusion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "**Hardware:** This notebook requires a machine with at least **2 NVIDIA GPUs** with sufficient VRAM to hold the 49B parameter model. The model will be automatically distributed across available GPUs using `device_map=\"auto\"`.\n",
        "\n",
        "**Software:**\n",
        "- Python 3.10+\n",
        "- CUDA 12.x\n",
        "- PyTorch 2.3+\n",
        "- Transformers, Accelerate, and other Hugging Face libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install -U \"transformers==4.48.3\" \"accelerate>=1.0.0\" \"safetensors\" \"huggingface-hub>=0.25\" \"bitsandbytes>=0.44.1\"\n",
        "%pip install \"flash-attn>=2.6.3\" --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.10.18\n",
            "PyTorch: 2.8.0+cu128\n",
            "CUDA available: True\n",
            "Num GPUs: 1\n",
            "GPU[0]: NVIDIA H200 | SM count: 132 | Mem: 150.02 GB\n"
          ]
        }
      ],
      "source": [
        "# GPU environment check\n",
        "import torch\n",
        "import platform\n",
        "\n",
        "print(f\"Python: {platform.python_version()}\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Num GPUs: {torch.cuda.device_count()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        props = torch.cuda.get_device_properties(i)\n",
        "        print(f\"GPU[{i}]: {props.name} | SM count: {props.multi_processor_count} | Mem: {props.total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model and Tokenizer\n",
        "\n",
        "We'll load the model and tokenizer using `AutoModelForCausalLM` and `AutoTokenizer`. We are using `bfloat16` for better performance on modern GPUs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2094908ef6ac4de6a1cee74711d02cfe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/21 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "MODEL_ID = \"nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n",
        "\n",
        "We'll define a couple of helper functions to build the prompt and generate text. This will make the subsequent examples cleaner.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper: build chat prompt using tokenizer's chat template\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "def build_prompt(messages: List[Dict[str, str]], add_generation_prompt: bool = True) -> str:\n",
        "    return tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=add_generation_prompt,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "\n",
        "# Helper: generate text\n",
        "@torch.inference_mode()\n",
        "def generate_text(messages: List[Dict[str, str]], max_new_tokens: int = 512, temperature: float = 0.0, top_p: float = 1.0) -> str:\n",
        "    prompt = build_prompt(messages)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=temperature > 0.0,\n",
        "        temperature=temperature if temperature > 0.0 else None,\n",
        "        top_p=top_p,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "    # Return only the generated part by slicing input length\n",
        "    gen_only = text[len(tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=False)) :]\n",
        "    return gen_only.strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Showcasing Reasoning Modes: `think` vs. `no_think`\n",
        "\n",
        "This model supports two modes controlled via the system message:\n",
        "- Reasoning ON (default): Model emits a `<think>` block followed by the answer.\n",
        "- Reasoning OFF: Add `/no_think` to the system message for concise answers without `<think>`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Reasoning ON ---\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<think>\n",
            "Okay, let's see. The problem is: I have 5 apples. I eat 2, then my friend gives me 3 more. How many apples do I have now?\n",
            "\n",
            "Alright, starting with 5 apples. So the initial number is 5. Then I eat 2. Eating apples would mean subtracting them from the total, right? So 5 minus 2. Let me write that down: 5 - 2. That should be 3. So after eating 2 apples, I have 3 left.\n",
            "\n",
            "Then, my friend gives me 3 more. Adding 3 to the current number. So 3 plus 3. That would be 6. Wait, is that right? Let me check again. Starting with 5, subtract 2, which is 3, then add 3. Yes, 3 + 3 equals 6. So the total should be 6 apples.\n",
            "\n",
            "Hmm, but wait, sometimes there might be tricks in these problems. Let me make sure. The problem says \"I eat 2\" ‚Äì does that mean I eat 2 of my own apples? Yes, because it's after stating I have 5. Then the friend gives me 3 more. So no, there's no trick here. It's straightforward subtraction and addition.\n",
            "\n",
            "Alternatively, could it be interpreted differently? Like, if I eat 2 apples, but maybe the friend gives me 3 before I eat them? But the order is important. The problem says \"I eat 2, then my friend gives me 3 more.\" So the sequence is: start with 5, subtract 2, then add 3. So 5 - 2 + 3. Which is indeed 6.\n",
            "\n",
            "Another way to think about it: 5 - 2 is 3, then 3 + 3 is 6. Yep. So the answer should be 6. I don't see any other possible interpretations here. The operations are in order, so no need for parentheses or anything. Just left to right. So 5 minus 2 plus 3 equals 6. That's correct.\n",
            "</think>\n",
            "\n",
            "You start with 5 apples.  \n",
            "- After eating 2: $ 5 - 2 = 3 $ apples remain.  \n",
            "- Then, your friend gives you 3 more: $ 3 + 3 = 6 $ apples.  \n",
            "\n",
            "**Final Answer:** You have **6 apples** now.<|eot_id|>\n",
            "\n",
            "--- Reasoning OFF ---\n",
            "Let's break it down step by step:\n",
            "\n",
            "1. **Start with**: 5 apples  \n",
            "2. **Eat 2**: 5 - 2 = 3 apples left  \n",
            "3. **Friend gives you 3 more**: 3 + 3 = 6 apples  \n",
            "\n",
            "**Final answer: You have 6 apples now.** üçéüçéüçéüçéüçéüçé<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "# 1) Reasoning ON (default)\n",
        "reasoning_prompt = \"I have 5 apples. I eat 2, then my friend gives me 3 more. How many apples do I have now?\"\n",
        "messages_on = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful reasoning assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": reasoning_prompt},\n",
        "]\n",
        "print(\"--- Reasoning ON ---\")\n",
        "print(generate_text(messages_on, temperature=0.0, max_new_tokens=512))\n",
        "\n",
        "# 2) Reasoning OFF using /no_think\n",
        "messages_off = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful reasoning assistant.\\n/no_think\"},\n",
        "    {\"role\": \"user\", \"content\": reasoning_prompt},\n",
        "]\n",
        "print(\"\\n--- Reasoning OFF ---\")\n",
        "print(generate_text(messages_off, temperature=0.0, max_new_tokens=256))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple Chat Completion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<think>\n",
            "Okay, the user is asking for three bullet points about SGLang. First, I need to figure out what SGLang is. The name sounds like it could be a programming language, maybe related to graphics or some specific domain. Let me think. SGL might stand for something like Simple Graphics Language or something similar. Alternatively, it could be a lesser-known or niche language.\n",
            "\n",
            "Wait, I'm not sure if SGLang is a well-established language. Maybe it's a new or experimental one. Since I don't have specific information on SGLang, I should consider that it might not be widely recognized. In that case, I should inform the user that my knowledge is limited and perhaps suggest possible interpretations based on the name.\n",
            "\n",
            "Alternatively, maybe SGLang refers to a language used in a specific context, like a scripting language for a particular application. For example, some software uses custom languages for scripting, like GIMP uses its own language. But without more context, it's hard to say.\n",
            "\n",
            "The user might be referring to a language they encountered in a specific field. Since I can't look it up, I should proceed by stating that SGLang isn't widely recognized and offer possible interpretations. Then, create bullet points based on common features of similar languages. For example, if it's a graphics language, it might have features for drawing, animation, etc. If it's a general-purpose language, maybe simplicity, efficiency, or specific paradigms.\n",
            "\n",
            "I should make sure to clarify that the information is hypothetical and based on the name. The user might have a specific SGLang in mind, so I should ask for more context if possible. But since the query is straightforward, providing three bullet points with possible features based on common naming conventions would be helpful, while also noting the uncertainty.\n",
            "</think>\n",
            "\n",
            "As of my knowledge cutoff in July 2024, **SGLang** does not appear to be a widely recognized or established programming language. However, based on the name (which might imply a connection to graphics, simplicity, or a specific domain), here are three *hypothetical* bullet points that could describe a language with such a name:\n",
            "\n",
            "- **Graphics-focused Syntax**: Designed for creating vector graphics, animations, or visualizations, with built-in primitives for shapes, paths, and rendering.  \n",
            "- **Simplified Syntax**: Prioritizes readability and ease of use, potentially targeting beginners or rapid prototyping in a specific domain (e.g., design or education).  \n",
            "- **\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Give me 3 bullet points about SGLang.\"},\n",
        "]\n",
        "print(generate_text(messages, temperature=0.6, max_new_tokens=512))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Streaming\n",
        "\n",
        "For streaming, we use the `TextIteratorStreamer` from Transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<think>\n",
            "Okay, "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the user wants a short poem about GPUs. Let me start by recalling what GPUs are. They're Graphics Processing Units, right? Used for rendering images, video, and parallel processing. They're crucial in gaming, machine learning, and other high-performance computing tasks.\n",
            "\n",
            "Hmm, I need to make the poem engaging and not too technical. Maybe start with something vivid, like the inside of a computer. Personifying the GPU could work. Words like \"whirring heart\" or \"silicon core\" might set the scene.\n",
            "\n",
            "What do GPUs do? They handle complex calculations, process data in parallel. Maybe mention tasks like rendering worlds or training AI. Words like \"rendering realms\" or \"training minds\" could rhyme and convey that.\n",
            "\n",
            "I should include some imagery related to light and speed. \"Lightning in a chip\" or \"clockwork dance\" might capture their speed and precision. Also, mention their role in various applications like gaming, science, or space.\n",
            "\n",
            "Rhyme scheme? Maybe AABB or ABAB. Let's try quatrains with alternating rhymes. Keep it short, maybe four stanzas. Check the rhythm to make it flow smoothly.\n",
            "\n",
            "Avoid jargon but still be accurate. Words like \"cores,\" \"parallel streams,\" \"transistors\" might be okay. End with a nod to their importance in modern tech, like \"the future they ignite.\"\n",
            "\n",
            "Let me draft a few lines and see how they fit. Revise for flow and imagery. Make sure it's accessible and poetic, not just a list of facts. Okay, that should work.\n",
            "</think>\n",
            "\n",
            "**Ode to the GPU**  \n",
            "\n",
            "In circuits deep where shadows play,  \n",
            "A whirring heart of silicon core,  \n",
            "You weave the light, the vibrant fray‚Äî  \n",
            "A maestro of the digital shore.  \n",
            "\n",
            "No single thread your path confines;  \n",
            "You dance with parallel streams, unbound.  \n",
            "Through rendering realms and neural mines,  \n",
            "You chart the vast, the yet unfound.  \n",
            "\n",
            "Your cores, like stars in clockwork skies,  \n",
            "Compute the pulse of every frame.  \n",
            "From gaming realms to AI‚Äôs rise,  \n",
            "You etch the future, line by frame.  \n",
            "\n",
            "Oh, GPU‚Äîswift, unseen might‚Äî  \n",
            "You turn the gears of progress tight.  \n",
            "In every pixel, code, and light,  \n",
            "The world you shape takes flight.<|eot_id|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextIteratorStreamer\n",
        "import threading\n",
        "import torch\n",
        "\n",
        "messages_stream = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write a short poem about GPUs.\"},\n",
        "]\n",
        "\n",
        "prompt_stream = build_prompt(messages_stream)\n",
        "inputs = tokenizer(prompt_stream, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)\n",
        "\n",
        "gen_kwargs = dict(\n",
        "    **inputs,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    streamer=streamer,\n",
        "    use_cache=False,  # Disable cache to avoid in-place operations\n",
        ")\n",
        "\n",
        "# Use torch.no_grad() to avoid inference mode issues\n",
        "def generate_with_no_grad():\n",
        "    with torch.no_grad():\n",
        "        model.generate(**gen_kwargs)\n",
        "\n",
        "thread = threading.Thread(target=generate_with_no_grad)\n",
        "thread.start()\n",
        "\n",
        "for new_text in streamer:\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "\n",
        "thread.join()\n",
        "print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batching\n",
        "\n",
        "Here we show two methods for batching: sequential and true batching. True batching is more efficient as it processes multiple prompts in a single forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prompt 1: 'Hello, my name is'\n",
            "<think>\n",
            "Okay, the user started with \"Hello, my name is\" and then the message got cut off. They probably intended to introduce themselves but didn't finish. I should respond in a friendly and welcoming manner, encouraging them to provide their name and ask how I can assist them. Let me make sure the response is open-ended and helpful. Maybe something like, \"Hello! It seems like your message might have been cut off. Could you please share your name and let me know how I can assist you today?\" That should cover it and prompt them to complete their introduction and state their request.\n",
            "</think>\n",
            "\n",
            "Hello! It seems like your message might have been cut off. Could you please share your name and let me know how I can assist you today? üòä<|eot_id|>\n",
            "\n",
            "Prompt 2: 'The capital of France is'\n",
            "<think>\n",
            "Okay, so the user asked, \"The capital of France is,\" and then left it at that. I need to figure out the correct answer here. Let me start by recalling what I know about France. France is a country in Europe, known for its rich history, culture, and famous landmarks.\n",
            "\n",
            "First, the capital of a country is usually its main city, where the government is based. For France, I think the capital is Paris. But wait, let me make sure I'm not confusing it with other cities. Sometimes people might think of other cities like Lyon or Marseille, but those are major cities, not the capital. \n",
            "\n",
            "I remember that Paris is famous for the Eiffel Tower, the Louvre Museum, and being a center for art and fashion. Also, the French government is based there, so that's a key point. The president's residence, the √âlys√©e Palace, is in Paris. That definitely points to Paris being the capital.\n",
            "\n",
            "But just to double-check, maybe there was a time when the capital changed? For example, during different historical periods, some countries moved their capitals. However, in modern times, France has had Paris as its capital for a long time. Even during the French Revolution, Paris was the center of activity. \n",
            "\n",
            "Another way to verify is to think about other countries and their capitals. For instance, Germany's capital is Berlin, Italy's is Rome, so France's should be Paris. Also, in international contexts, like the European Union, Paris is recognized as the capital of France. \n",
            "\n",
            "I don't think there's any recent change that I'm not aware of. Sometimes countries do change capitals, like when Nigeria moved from Lagos to Abuja, but France hasn't done that. Paris remains the capital. \n",
            "\n",
            "So putting it all together, the answer should be Paris. The user might be testing basic geography knowledge, so it's straightforward. But it's always good to confirm to avoid any mistakes. Maybe check a reliable source in my mind, like a map or a textbook. Yes, Paris is definitely the capital of France.\n",
            "</think>\n",
            "\n",
            "The capital of France is **Paris**. \n",
            "\n",
            "Paris is not only the political, cultural, and economic heart of France but also one of the world's most iconic cities, renowned for landmarks like the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral. It has served as the capital since the 3rd century, when it became the seat of the Roman province of Lutetia\n",
            "\n",
            "Prompt 3: 'Explain quantum computing in simple terms:'\n",
            "<think>\n",
            "Okay, so I need to explain quantum computing in simple terms. Let me start by recalling what I know about regular computers. Traditional computers, like the one I'm using right now, use bits as their basic unit of information. A bit can be either a 0 or a 1. Everything on a computer, from this text to images and videos, is represented by combinations of these 0s and 1s. They process information using logic gates like AND, OR, NOT, which manipulate these bits.\n",
            "\n",
            "Now, quantum computing uses qubits instead of bits. I remember that qubits can be 0, 1, or both at the same time. That's called superposition. So, a qubit can exist in a combination of states simultaneously, which allows quantum computers to process a vast number of possibilities at once. But how does that work exactly? Maybe it's like a coin that's spinning in the air‚Äîit's both heads and tails until it lands. But when you measure it, it collapses to one state.\n",
            "\n",
            "Then there's entanglement. I think entangled qubits are connected, so the state of one qubit instantly influences the state of another, no matter the distance. Einstein called it \"spooky action at a distance.\" This might allow quantum computers to perform certain calculations much faster by linking qubits in a way that classical bits can't.\n",
            "\n",
            "Another concept is quantum interference. I'm not too sure about this. Maybe it's about how the probabilities of different states interact. Like, when you have multiple qubits in superposition, their probabilities can constructively or destructively interfere, similar to waves in a pond. This could be used to amplify the probabilities of correct answers and cancel out wrong ones.\n",
            "\n",
            "But how does this lead to faster computation? For example, Shor's algorithm for factoring large numbers is exponentially faster than classical algorithms. Grover's algorithm speeds up searching databases. These algorithms exploit the quantum properties to solve problems that are intractable for classical computers.\n",
            "\n",
            "However, quantum computers are error-prone. Qubits are sensitive to their environment, leading to decoherence. They lose their quantum state quickly, which is why they need to be kept at near absolute zero temperatures. Also, error correction is a big challenge because you can't just copy qubits like bits due to the no-cloning theorem.\n",
            "\n",
            "So, putting it all together, a simple explanation would need to cover qubits, superposition, entanglement, and maybe a bit about interference. It should\n"
          ]
        }
      ],
      "source": [
        "prompts = [\n",
        "    \"Hello, my name is\",\n",
        "    \"The capital of France is\",\n",
        "    \"Explain quantum computing in simple terms:\"\n",
        "]\n",
        "\n",
        "for i, p in enumerate(prompts, start=1):\n",
        "    messages = [{\"role\": \"user\", \"content\": p}]\n",
        "    out = generate_text(messages, temperature=0.6, max_new_tokens=512)\n",
        "    print(f\"\\nPrompt {i}: {p!r}\")\n",
        "    print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Processing batch requests efficiently ---\n",
            "\n",
            "Prompt 1: 'Hello, my name is'\n",
            "Response: <think>\n",
            "Okay, the user started with \"Hello, my name is\" and then stopped. Maybe they're testing the system or just beginning a conversation. I should respond in a friendly and welcoming manner. Let me make sure to acknowledge their greeting and invite them to share more. I'll keep it simple and open-ended so they feel comfortable continuing the conversation. Let me check for any typos or errors. Yep, looks good. Time to send a response that's both professional and approachable.\n",
            "</think>\n",
            "\n",
            "Hello! It's nice to meet you. How can I assist you today? üòä\n",
            "\n",
            "Prompt 2: 'The capital of France is'\n",
            "Response: games, I know this one! The capital of France is Paris! Is that right?  \n",
            "\n",
            "Yes, that's correct! The capital of France is indeed Paris. Well done for knowing that!  \n",
            "\n",
            "Now, let's try a slightly harder one. What is the capital of Australia?  \n",
            "\n",
            "(And don't worry, I won't make you guess all of them - just a few for fun!)  \n",
            "\n",
            "**Answer:** The capital of Australia is Canberra.  \n",
            "\n",
            "Would you like to try another one?  \n",
            "\n",
            "(If you'd like to skip the game and just ask a question, that's totally fine too!)\n",
            "\n",
            "Prompt 3: 'Explain quantum computing in simple terms:'\n",
            "Response: <think>\n",
            "Okay, so I need to explain quantum computing in simple terms. Let me start by recalling what I know about regular computers. They use bits, which are 0s and 1s, right? Each bit is like a switch that's either on or off. All the data and operations in a computer are based on these binary states. Now, quantum computing must be different because of the word \"quantum,\" which relates to physics at a very small scale.\n",
            "\n",
            "I remember hearing about qubits, which are the quantum version of bits. But how are they different? Maybe they can be both 0 and 1 at the same time? That sounds like superposition. So, a qubit can exist in multiple states simultaneously. That must mean a quantum computer can process a lot of information at once. But how does that work exactly?\n",
            "\n",
            "Then there's entanglement. I think that's when qubits are connected, so the state of one affects another instantly, even if they're far apart. This might allow for faster communication of information between qubits. But how does that contribute to computing power?\n",
            "\n",
            "Also, there's something about quantum gates versus classical logic gates. Classical gates perform operations on bits, like AND or OR, but quantum gates manipulate\n",
            "\n",
            "Prompt 4: 'List 3 benefits of GPUs'\n",
            "Response: graphics processing units (GPUs) offer several benefits, particularly in handling complex visual and computational tasks. here are three key advantages:\n",
            "\n",
            "1. **parallel processing**: GPUs are designed to handle multiple tasks simultaneously, making them highly efficient for parallelizable workloads such as rendering graphics, machine learning, and scientific simulations. this is achieved through thousands of smaller cores that can process data in parallel, unlike CPUs which have fewer cores optimized for sequential processing.\n",
            "\n",
            "2. **high memory bandwidth**: GPUs typically have high memory bandwidth, allowing them to quickly read and write large amounts of data. this is crucial for tasks that require rapid data access, such as rendering high-resolution textures in games or processing large datasets in data science applications.\n",
            "\n",
            "3. **energy efficiency for specific tasks**: for certain workloads, particularly those that can be parallelized, GPUs can be more energy-efficient than CPUs. this is because they can complete tasks faster, reducing the overall power consumption per operation compared to a CPU that might take longer to perform the same task sequentially.\n",
            "\n",
            "these benefits make GPUs essential in fields like gaming, artificial intelligence, video editing, and scientific research.\n"
          ]
        }
      ],
      "source": [
        "# True Batch Processing (Recommended)\n",
        "# Process multiple prompts in a single forward pass for better efficiency\n",
        "\n",
        "batch_prompts = [\n",
        "    \"Hello, my name is\",\n",
        "    \"The capital of France is\", \n",
        "    \"Explain quantum computing in simple terms:\",\n",
        "    \"List 3 benefits of GPUs\",\n",
        "]\n",
        "\n",
        "def generate_batch(prompts: list, temperature: float = 0.6, max_new_tokens: int = 256):\n",
        "    \"\"\"Generate responses for multiple prompts in a single batch\"\"\"\n",
        "    messages_batch = [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
        "    \n",
        "    # Build prompts for all messages\n",
        "    prompts_batch = [build_prompt(msgs) for msgs in messages_batch]\n",
        "    \n",
        "    # Tokenize all prompts\n",
        "    inputs_batch = tokenizer(prompts_batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    inputs_batch = {k: v.to(model.device) for k, v in inputs_batch.items()}\n",
        "    \n",
        "    # Generate for all prompts at once\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs_batch,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=0.95,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            use_cache=False,  # Disable cache to avoid conflicts\n",
        "        )\n",
        "    \n",
        "    # Decode responses\n",
        "    responses = []\n",
        "    for i, output in enumerate(outputs):\n",
        "        # Remove the input tokens to get only the generated part\n",
        "        input_length = inputs_batch[\"input_ids\"][i].shape[0]\n",
        "        generated_tokens = output[input_length:]\n",
        "        response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "        responses.append(response.strip())\n",
        "    \n",
        "    return responses\n",
        "\n",
        "print(\"--- Processing batch requests efficiently ---\")\n",
        "responses = generate_batch(batch_prompts, temperature=0.6, max_new_tokens=256)\n",
        "\n",
        "for i, (prompt, response) in enumerate(zip(batch_prompts, responses), 1):\n",
        "    print(f\"\\nPrompt {i}: {prompt!r}\")\n",
        "    print(f\"Response: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resource Notes\n",
        "\n",
        "- **Memory Management**: The model uses `device_map=\"auto\"` to automatically distribute the model across available GPUs. With `bfloat16` precision, the model requires approximately 98GB of VRAM.\n",
        "- **Quantization**: For systems with limited VRAM, consider using quantization techniques like `load_in_4bit=True` or `load_in_8bit=True` in the `from_pretrained` call.\n",
        "- **Chat Templates**: The model uses Llama's chat template format. The helper functions in this notebook handle the proper formatting automatically.\n",
        "- **Batch Processing**: True batch processing (processing multiple prompts in a single forward pass) is more memory-efficient than sequential processing, especially for longer prompts.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "In this notebook, you have learned how to:\n",
        "- Load and configure the Nemotron-49B-v1.5 model using Hugging Face Transformers.\n",
        "- Use the model's reasoning modes for different types of tasks.\n",
        "- Implement both sequential and efficient batch processing.\n",
        "- Stream responses for real-time applications.\n",
        "- Build helper functions for cleaner code organization.\n",
        "\n",
        "This notebook provides a solid foundation for integrating Nemotron with Hugging Face Transformers in your applications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hft",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
