{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SAw3-nW7dD6"
      },
      "source": [
        "# Running Nemotron-49B-v1.5 with SGLang on NVIDIA GPUs\n",
        "\n",
        "This notebook provides a comprehensive guide on how to run the `Nemotron-49B-v1.5` model using SGLang's high-performance, OpenAI-compatible server.\n",
        "\n",
        "This notebook will cover:\n",
        "- Setting up the SGLang server for the Nemotron model.\n",
        "- Performing basic and streaming chat completions.\n",
        "- Running batch inference for multiple prompts.\n",
        "- Showcasing the model's `think` vs `no_think` reasoning modes.\n",
        "\n",
        "#### Launch on NVIDIA Brev\n",
        "You can simplify the environment setup by using [NVIDIA Brev](https://developer.nvidia.com/brev). Click the button below to launch this project on a Brev instance with the necessary dependencies pre-configured.\n",
        "\n",
        "Once deployed, click on the \"Open Notebook\" button to get started with this guide.\n",
        "\n",
        "[![Launch on Brev](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-32vt7HcQjCUpafGyquLZwJdIm8F)\n",
        "\n",
        "- Model card: [nvidia/Llama-3.3-Nemotron-Super-49B-v1.5](https://huggingface.co/nvidia/Llama-3.3-Nemotron-Super-49B-v1.5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "- [Prerequisites](#Prerequisites)\n",
        "- [Setup](#Setup)\n",
        "- [Start SGLang Server](#Start-SGLang-Server)\n",
        "- [Client Setup](#Client-Setup)\n",
        "- [Showcasing Reasoning Modes: `think` vs. `no_think`](#Showcasing-Reasoning-Modes:-`think`-vs.-`no_think`)\n",
        "- [Chat Completion Examples](#Chat-Completion-Examples)\n",
        "- [Batching](#Batching)\n",
        "- [Asynchronous-Batching](#Asynchronous-Batching)\n",
        "- [Direct Interaction with `curl`](#Direct-Interaction-with-`curl`)\n",
        "- [Resource-Notes](#Resource-Notes)\n",
        "- [Conclusion](#Conclusion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCbkowhF7dD7"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "**Hardware:** This notebook is configured to run on a machine with at least **2 GPUs** and sufficient VRAM to hold the 49B parameter model. If your hardware is different, you may need to adjust the `--tp` (tensor parallelism) flag in the server launch command below.\n",
        "\n",
        "**Software:**\n",
        "- Python 3.10+\n",
        "- CUDA 12.x\n",
        "- PyTorch 2.3+\n",
        "- Latest SGLang\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fP3Wmvwd7dD7"
      },
      "outputs": [],
      "source": [
        "# Install SGLang and useful extras (run once per env)\n",
        "%pip install --upgrade pip\n",
        "%pip install uv\n",
        "%uv pip install \"sglang[all]>=0.5.3rc0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dzubt9pL7dD7",
        "outputId": "fae4c77a-e740-47f2-a84d-cd449f1f3e6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "Num GPUs: 1\n",
            "GPU[0]: NVIDIA H200\n"
          ]
        }
      ],
      "source": [
        "# GPU environment check\n",
        "import torch\n",
        "import platform\n",
        "\n",
        "print(f\"Python: {platform.python_version()}\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Num GPUs: {torch.cuda.device_count()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        props = torch.cuda.get_device_properties(i)\n",
        "        print(f\"GPU[{i}]: {props.name} | SM count: {props.multi_processor_count} | Mem: {props.total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSbIp6b27dD8"
      },
      "source": [
        "## Start SGLang Server\n",
        "\n",
        "SGLang runs as a separate server process. The following cell starts the server. You can also run this command in a terminal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIynvhsX7dD8",
        "outputId": "7ec9281d-6e59-4ef6-b56b-31699de2587b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "All deep_gemm operations loaded successfully!\n",
            "W0917 22:57:39.794000 418063 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W0917 22:57:39.794000 418063 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "/home/ubuntu/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "/home/ubuntu/miniconda3/envs/sgl/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "All deep_gemm operations loaded successfully!\n",
            "W0917 22:57:46.521000 418277 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W0917 22:57:46.521000 418277 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
            "All deep_gemm operations loaded successfully!\n",
            "W0917 22:57:46.709000 418278 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "W0917 22:57:46.709000 418278 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[2025-09-17 22:57:48] CUDA-fused xIELU not available (No module named 'xielu') ‚Äì falling back to a Python version.\n",
            "For CUDA xIELU (experimental), `pip install git+https://github.com/nickjbrowning/XIELU`\n",
            "[2025-09-17 22:57:48] MOE_RUNNER_BACKEND is not initialized, using triton backend\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/21 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:   5% Completed | 1/21 [00:00<00:12,  1.60it/s]\n",
            "Loading safetensors checkpoint shards:  10% Completed | 2/21 [00:01<00:12,  1.47it/s]\n",
            "Loading safetensors checkpoint shards:  14% Completed | 3/21 [00:02<00:13,  1.38it/s]\n",
            "Loading safetensors checkpoint shards:  19% Completed | 4/21 [00:02<00:12,  1.34it/s]\n",
            "Loading safetensors checkpoint shards:  24% Completed | 5/21 [00:03<00:11,  1.35it/s]\n",
            "Loading safetensors checkpoint shards:  29% Completed | 6/21 [00:04<00:10,  1.37it/s]\n",
            "Loading safetensors checkpoint shards:  33% Completed | 7/21 [00:05<00:10,  1.40it/s]\n",
            "Loading safetensors checkpoint shards:  38% Completed | 8/21 [00:05<00:09,  1.43it/s]\n",
            "Loading safetensors checkpoint shards:  43% Completed | 9/21 [00:06<00:08,  1.44it/s]\n",
            "Loading safetensors checkpoint shards:  48% Completed | 10/21 [00:07<00:07,  1.45it/s]\n",
            "Loading safetensors checkpoint shards:  52% Completed | 11/21 [00:07<00:06,  1.47it/s]\n",
            "Loading safetensors checkpoint shards:  57% Completed | 12/21 [00:08<00:06,  1.45it/s]\n",
            "Loading safetensors checkpoint shards:  62% Completed | 13/21 [00:09<00:05,  1.46it/s]\n",
            "Loading safetensors checkpoint shards:  67% Completed | 14/21 [00:09<00:04,  1.44it/s]\n",
            "Loading safetensors checkpoint shards:  71% Completed | 15/21 [00:10<00:04,  1.42it/s]\n",
            "Loading safetensors checkpoint shards:  76% Completed | 16/21 [00:11<00:03,  1.44it/s]\n",
            "Loading safetensors checkpoint shards:  81% Completed | 17/21 [00:11<00:02,  1.41it/s]\n",
            "Loading safetensors checkpoint shards:  86% Completed | 18/21 [00:12<00:02,  1.41it/s]\n",
            "Loading safetensors checkpoint shards:  90% Completed | 19/21 [00:13<00:01,  1.42it/s]\n",
            "Loading safetensors checkpoint shards:  95% Completed | 20/21 [00:14<00:00,  1.40it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 21/21 [00:14<00:00,  1.50it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 21/21 [00:14<00:00,  1.43it/s]\n",
            "\n",
            "Capturing batches (bs=1 avail_mem=38.83 GB): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.18it/s]\n",
            "\n",
            "\n",
            "                    NOTE: Typically, the server runs in a separate terminal.\n",
            "                    In this notebook, we run the server and notebook code together, so their outputs are combined.\n",
            "                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.\n",
            "                    To reduce the log length, we set the log level to warning for the server, the default log level is info.\n",
            "                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.\n",
            "                    \n"
          ]
        }
      ],
      "source": [
        "from sglang.test.doc_patch import launch_server_cmd\n",
        "from sglang.utils import wait_for_server, print_highlight, terminate_process\n",
        "\n",
        "# This is equivalent to running the following command in your terminal\n",
        "# python -m sglang.launch_server --model-path \"nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\" --host 0.0.0.0 --trust-remote-code\n",
        "\n",
        "server_process, port = launch_server_cmd(\n",
        "    \"\"\"\n",
        "python3 -m sglang.launch_server --model-path nvidia/Llama-3_3-Nemotron-Super-49B-v1_5 \\\n",
        " --host 0.0.0.0 --log-level warning --trust-remote-code\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "wait_for_server(f\"http://localhost:{port}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Client Setup\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "# The model name we used when launching the server.\n",
        "SERVED_MODEL_NAME = \"nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\"\n",
        "\n",
        "BASE_URL = f\"http://localhost:{port}/v1\"\n",
        "API_KEY = \"EMPTY\"  # SGLang server doesn't require an API key by default\n",
        "\n",
        "client = OpenAI(base_url=BASE_URL, api_key=API_KEY)\n",
        "print(f\"OpenAI client configured to use server at: {BASE_URL}\")\n",
        "print(f\"Using model: {SERVED_MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vAeKMx27dD8"
      },
      "source": [
        "## Showcasing Reasoning Modes: `think` vs. `no_think`\n",
        "\n",
        "As described in the [model card](https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5), this model has two distinct reasoning modes.\n",
        "\n",
        "1.  **Reasoning ON (`think` mode):** This is the default mode. The model first generates a `<think>` block where it outlines its step-by-step reasoning process before providing the final answer. This is ideal for complex, multi-step problems.\n",
        "2.  **Reasoning OFF (`no_think` mode):** This mode is activated by adding `/no_think` to the system prompt. The model provides a direct, concise answer without the preceding thought process. This is better for simple, instruction-following tasks where latency is a concern.\n",
        "\n",
        "Let's see this in action with a simple reasoning problem.\n",
        "\n",
        "# 1. Reasoning ON (Default Behavior)\n",
        "\n",
        "We'll send a multi-step problem with a standard system prompt. We expect to see the model's thought process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7oizKAN7dD8",
        "outputId": "d3be26be-68ba-41a3-fdc1-a4189dff4bf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Sending prompt with Reasoning ON ---\n",
            "\\n--- Response with Reasoning ON ---\n",
            "<think>\n",
            "Okay, let's see. The problem is: I have 5 apples. I eat 2, then my friend gives me 3 more. How many apples do I have now?\n",
            "\n",
            "Alright, starting with 5 apples. So the initial number is 5. Then I eat 2. Eating apples would mean subtracting them from the total, right? So 5 minus 2. Let me write that down: 5 - 2. That should be 3. So after eating 2, I have 3 apples left.\n",
            "\n",
            "Then, my friend gives me 3 more. So adding 3 to the current number. The current number after eating is 3, so adding 3 would be 3 + 3. That equals 6. So putting it all together: start with 5, subtract 2, add 3. So 5 - 2 + 3. Let me check the order of operations here. Subtraction and addition are at the same level, so we do them left to right. 5 - 2 is 3, then 3 + 3 is 6. Yep, that seems right.\n",
            "\n",
            "Wait, but sometimes people might get confused if there's a different order, but in this case, the operations are straightforward. So the answer should be 6. Let me think again. Original 5, eat 2, so 5-2=3. Then friend gives 3, so 3+3=6. Yep, that's correct. I don't think there's any trick here. Maybe someone could misread the problem, but as per the steps given, it's simple subtraction and addition. So the final answer is 6.\n",
            "</think>\n",
            "\n",
            "You start with 5 apples.  \n",
            "- After eating 2: $ 5 - 2 = 3 $ apples remain.  \n",
            "- Then, your friend gives you 3 more: $ 3 + 3 = 6 $ apples.  \n",
            "\n",
            "**Final Answer:** You have **6 apples** now.\n"
          ]
        }
      ],
      "source": [
        "reasoning_prompt = \"I have 5 apples. I eat 2, then my friend gives me 3 more. How many apples do I have now?\"\n",
        "\n",
        "print(\"--- Sending prompt with Reasoning ON ---\")\n",
        "response_on = client.chat.completions.create(\n",
        "    model=SERVED_MODEL_NAME,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful reasoning assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": reasoning_prompt}\n",
        "    ],\n",
        "    temperature=0.0,\n",
        "    max_tokens=512,\n",
        ")\n",
        "\n",
        "print(\"\\\\n--- Response with Reasoning ON ---\")\n",
        "print(response_on.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuxUIKsV7dD8",
        "outputId": "dc3a1e60-8f72-49bc-966d-572aebdd733b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Sending prompt with Reasoning OFF ---\n",
            "\\n--- Response with Reasoning OFF ---\n",
            "Let's break it down step by step:\n",
            "\n",
            "1. **Start with**: 5 apples  \n",
            "2. **Eat 2**: 5 - 2 = 3 apples left  \n",
            "3. **Friend gives you 3 more**: 3 + 3 = 6 apples  \n",
            "\n",
            "**Final answer**: You now have **6 apples**. üçéüçéüçéüçéüçéüçé\n"
          ]
        }
      ],
      "source": [
        "# 2. Reasoning OFF (using /no_think)\n",
        "\n",
        "# Now, we'll send the exact same prompt, but this time we add `/no_think` to the system message. We expect a direct answer without the `<think>` block.\n",
        "\n",
        "print(\"--- Sending prompt with Reasoning OFF ---\")\n",
        "response_off = client.chat.completions.create(\n",
        "    model=SERVED_MODEL_NAME,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful reasoning assistant.\\\\n/no_think\"},\n",
        "        {\"role\": \"user\", \"content\": reasoning_prompt}\n",
        "    ],\n",
        "    temperature=0.0,\n",
        "    max_tokens=512,\n",
        ")\n",
        "\n",
        "print(\"\\\\n--- Response with Reasoning OFF ---\")\n",
        "print(response_off.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JBsWXEr7dD8"
      },
      "source": [
        "## Chat Completion Examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic Chat Completion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIxcFI577dD8",
        "outputId": "aaf75d87-21bf-4721-e102-90493ef1d3b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Simple Chat Completion ===\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<think>\n",
            "Okay, the user is asking for three bullet points about SGLang. First, I need to recall what SGLang is. From what I remember, SGLang might be related to programming or a specific language. Wait, SGLang could stand for something like \"Simple Graphics Language\" or maybe it's a domain-specific language. Let me check my knowledge base.\n",
            "\n",
            "Hmm, I think SGLang is a lightweight programming language designed for educational purposes. It's used to teach programming concepts, especially in the context of graphics or game development. It might have a simple syntax to make it accessible for beginners. Also, SGLang could be used in specific educational platforms or tools. Another point might be that it's used for creating simple games or animations, which helps students grasp programming fundamentals through interactive projects. \n",
            "\n",
            "Wait, I should make sure I'm not confusing it with another language. Let me think. There's also a possibility that SGLang refers to a language used in a particular institution or course. Maybe it's part of a curriculum where students learn to build basic programs with immediate visual feedback. That would make sense for educational use. \n",
            "\n",
            "So, the three bullet points should highlight its purpose, key features, and typical use cases. Let\n",
            "\n",
            "\n",
            "=== Streaming Chat Completion ===\n",
            "<think>\n",
            "Okay, the user wants a short poem about GPUs. Let me start by recalling what GPUs are. They're graphics processing units, right? Used for rendering images, video, and now even more like machine learning and cryptocurrency mining. So, I need to capture their essence in a poem.\n",
            "\n",
            "First, think about the structure. A short poem, maybe four stanzas with a rhyme scheme. Let's go with quatrains, ABAB rhyme scheme. That's common and keeps it simple.\n",
            "\n",
            "What imagery can I use? Silicon, circuits, maybe something about light or speed. GPUs are powerful, so words like \"horses\" or \"engines\" might work. Also, they handle complex tasks, so terms like \"parallel streams\" or \"data tides\" could be good.\n",
            "\n",
            "I should mention their role in both gaming and other applications. Words like \"worlds unfold\" for gaming, and \"algorithms dance\" for more technical uses. Also, the physical aspect, like heat and light, maybe \"silicon heart\" or \"glow.\"\n",
            "\n",
            "Avoid being too technical but still accurate. Use metaphors. Let me start drafting lines. First stanza: Introduce the GPU as a powerful component. \"In circuits deep where silicon hearts race,\" ‚Äì that's a strong opening. Then something about processing power. \"A storm of cores in a lightning embrace,\" ‚Äì cores refer to the many processors in a GPU, and lightning for speed.\n",
            "\n",
            "Second stanza: Visuals and rendering. \"It paints the light in a billion hues,\" ‚Äì handling colors and images. \"Shaping the shadows, the dawn‚Äôs soft blues,\" ‚Äì creating realistic visuals. Then maybe something about movement. \"Each frame a leap, each pixel a spark,\" ‚Äì animation and detail. \"A universe built in the dark.\" ‚Äì creating entire worlds, like in games or simulations.\n",
            "\n",
            "Third stanza: Broader applications beyond gaming. \"From cryptic codes to the stars‚Äô cold gaze,\" ‚Äì cryptocurrency mining and scientific computations. \"It cracks the night with its number play,\" ‚Äì solving complex problems. \"A architect of dreams, both vast and small,\" ‚Äì creating and modeling. \"It maps the void, it gives form to all.\" ‚Äì giving structure to data and concepts.\n",
            "\n",
            "Fourth stanza: The physical aspect and the future. \"Yet in its glow, a whisper remains,\" ‚Äì the heat and light from the GPU. \"Of heat and time and silicon chains.\" ‚Äì the physical limitations. \"A servant bound to electric veins,\" ‚Äì dependent on power\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"=== Simple Chat Completion ===\")\n",
        "resp = client.chat.completions.create(\n",
        "    model=SERVED_MODEL_NAME,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Give me 3 bullet points about SGLang.\"}\n",
        "    ],\n",
        "    temperature=0.6,\n",
        "    max_tokens=512,\n",
        ")\n",
        "print(resp.choices[0].message.content)\n",
        "print(\"\\n\")  # Add a blank line for clarity\n",
        "\n",
        "# Streaming chat completion\n",
        "print(\"=== Streaming Chat Completion ===\")\n",
        "stream = client.chat.completions.create(\n",
        "    model=SERVED_MODEL_NAME,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a short poem about GPUs.\"}\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    max_tokens=512,\n",
        "    stream=True,\n",
        ")\n",
        "for chunk in stream:\n",
        "    delta = chunk.choices[0].delta\n",
        "    if delta and delta.content:\n",
        "        print(delta.content, end=\"\", flush=True)\n",
        "print(\"\\n\")  # Add a blank line after streaming output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXBTFN-A7dD9",
        "outputId": "a41c20f0-cf21-45d0-8d4c-a9e38bc67220"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prompt 1: 'Hello, my name is'\n",
            "<think>\n",
            "Okay, the user started with \"Hello, my name is\" and then the message was cut off. I need to figure out how to respond appropriately. Since the name wasn't provided, maybe they intended to type their name but got interrupted or there was a technical issue. I should acknowledge their greeting and invite them to complete their message. Let me make sure to keep the response friendly and open-ended so they feel comfortable finishing their thought. Something like, \"Hello! It seems like your message might have been cut off. Could you please share your name with me?\" That should work. I should check for any typos and ensure the tone is welcoming.\n",
            "</think>\n",
            "\n",
            "Hello! It seems like your message might have been cut off. Could you please share your name with me? I'd love to greet you properly! üòä\n",
            "\n",
            "Prompt 2: 'The capital of France is'\n",
            "<think>\n",
            "Okay, so the user asked, \"The capital of France is,\" and then left it open. I need to figure out the correct answer here. Let me start by recalling what I know about France. France is a country in Europe, known for its rich history, culture, and famous landmarks.\n",
            "\n",
            "First, I should remember the capital city. I think the capital of France is Paris. But wait, sometimes countries change their capitals, so I should double-check that. I don't recall any recent changes regarding France's capital. Paris has been the capital for a long time, right? Like, since the Middle Ages or something. \n",
            "\n",
            "Let me think of other major cities in France to make sure I'm not confusing it with another city. There's Marseille, Lyon, Bordeaux, Nice, but none of those are the capital. Paris is definitely the one that comes to mind. It's also home to famous landmarks like the Eiffel Tower, the Louvre Museum, and\n",
            "\n",
            "Prompt 3: 'Explain quantum computing in simple terms:'\n",
            "<think>\n",
            "Okay, so I need to explain quantum computing in simple terms. Let me start by recalling what I know about classical computers. They use bits, which are either 0 or 1. Everything a classical computer does is based on these binary states. Now, quantum computing must be different because it uses quantum bits, or qubits. But how exactly?\n",
            "\n",
            "I remember something about superposition. So a qubit can be both 0 and 1 at the same time, right? That means it can process a lot of information simultaneously. But how does that work in practice? Like, if you have two qubits, they can be in a superposition of four states (00, 01, 10, 11), and with more qubits, the number of states grows exponentially. That's why quantum computers might be faster for certain problems.\n",
            "\n",
            "Wait, but there's also entanglement. I think that's when qubits are linked together, so the state of\n"
          ]
        }
      ],
      "source": [
        "# Batch chat prompts\n",
        "from openai import OpenAI\n",
        "client = OpenAI(base_url=BASE_URL, api_key=\"dummy\")\n",
        "\n",
        "prompts = [\n",
        "    \"Hello, my name is\",\n",
        "    \"The capital of France is\",\n",
        "    \"Explain quantum computing in simple terms:\"\n",
        "]\n",
        "\n",
        "# Convert to messages for chat.completions\n",
        "messages_list = [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
        "\n",
        "responses = []\n",
        "for messages in messages_list:\n",
        "    out = client.chat.completions.create(\n",
        "        model=SERVED_MODEL_NAME,\n",
        "        messages=messages,\n",
        "        temperature=0.6,\n",
        "        max_tokens=512,\n",
        "    )\n",
        "    responses.append(out.choices[0].message.content)\n",
        "\n",
        "for i, (p, r) in enumerate(zip(prompts, responses), start=1):\n",
        "    print(f\"\\nPrompt {i}: {p!r}\")\n",
        "    print(r)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O66imx4I7dD9"
      },
      "source": [
        "### Asynchronous Batching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLJBpTe47dD9",
        "outputId": "44a7eb1c-3dab-4fa1-d09a-1b792d809c08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Sending batch requests concurrently ---\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- All responses received ---\n",
            "\\nPrompt 1: 'Hello, my name is'\n",
            "<think>\n",
            "Okay, the user started with \"Hello, my name is\" but didn't finish. I need to respond appropriately. Since they mentioned their name, I should ask them to provide the rest. Maybe they got cut off or are testing the system. I should keep it friendly and encouraging. Let me make sure to prompt them to complete their name so I can address them properly. Also, check for any typos or if they intended to write more. But since the message is cut off, the best approach is to ask for the rest of their name. Keep the response simple and welcoming.\n",
            "</think>\n",
            "\n",
            "Hello! It seems like your message got cut off. Could you please share the rest of your name with me? I'd love to know what to call you as we chat! üòä\n",
            "\\nPrompt 2: 'The capital of France is'\n",
            "<think>\n",
            "Okay, so the user asked, \"The capital of France is,\" and I need to figure out the answer. Let me start by recalling what I know about France. France is a country in Europe, known for the Eiffel Tower, the Louvre Museum, and its rich history. Now, the capital city of a country is usually the seat of government and where the main administrative offices are located. \n",
            "\n",
            "I remember that Paris is a major city in France. In fact, I think Paris is the capital. But wait, I should make sure I'm not confusing it with other cities. Sometimes countries change their capitals, but I don't think France has done that recently. Let me think of other French cities. There's Marseille, Lyon, Bordeaux, Nice, but none of those are capitals. The government of France is definitely based in Paris. The president's residence, the √âlys√©e Palace, is in Paris. Also, the headquarters of many international organizations are there, like UNESCO.\n",
            "\n",
            "Another way to confirm is to think about famous landmarks. The Eiffel Tower, Notre-Dame Cathedral, the Champs-√âlys√©es ‚Äì all of these are in Paris. If the capital were another city, those landmarks would probably be there instead. Also, when people talk about France in the news, especially regarding government matters, they often mention Paris. For example, during the Paris Agreement on climate change, the discussions were held in Paris.\n",
            "\n",
            "I should also consider if there's any historical context. Was there ever a time when the capital wasn't Paris? I think during the French Revolution, Paris was the center of activity, so it's been the capital for a long time. Before that, maybe other cities like Versailles? But Versailles is a suburb of Paris now, and while it was the seat of power under Louis XIV, the capital status might have shifted back to Paris after the revolution.\n",
            "\n",
            "To double-check, I can think of other countries and their capitals. For example, the capital of Italy is Rome, Germany is Berlin, Spain is Madrid. Following that pattern, France's capital should be Paris. Also, in international contexts, like the European Union, Paris is recognized as the capital of France.\n",
            "\n",
            "Wait, but just to be thorough, is there any chance that the capital is a different city? For example, sometimes people confuse the capital with the largest city. In France, Paris is both the largest city and the capital, so that aligns. In some countries, like Brazil, the capital is\n",
            "\\nPrompt 3: 'Explain quantum computing in simple terms:'\n",
            "<think>\n",
            "Okay, so I need to explain quantum computing in simple terms. Let me start by recalling what I know about regular computers. They use bits, which are either 0s or 1s, right? Everything a computer does is based on these binary states. But quantum computing uses qubits instead. I remember that qubits can be both 0 and 1 at the same time because of superposition. But how exactly does that work?\n",
            "\n",
            "Wait, superposition is like a spinning coin, not just heads or tails. So a qubit can be in a combination of both states until it's measured. That probably allows quantum computers to process a lot of information simultaneously. But how does that translate into computing power?\n",
            "\n",
            "Then there's entanglement. I think that's when qubits are linked together, so the state of one affects the other instantly, no matter the distance. This might help in processing information in a more interconnected way. But I'm not sure how that's used in actual computations.\n",
            "\n",
            "Also, quantum gates manipulate qubits, similar to logic gates in classical computers, but they work with probabilities. So quantum algorithms can solve certain problems faster, like factoring large numbers or simulating molecules. But why are they faster? Maybe because they explore many possibilities at once?\n",
            "\n",
            "Wait, but I should make sure I'm not mixing things up. Let me structure this. Start with classical bits vs qubits, explain superposition and entanglement, then maybe mention some applications where quantum computers excel, like Shor's algorithm for factoring or Grover's for searching. But keep it simple, avoid jargon.\n",
            "\n",
            "I should also note that quantum computers aren't just faster versions of classical ones; they're better for specific tasks. They're not going to replace regular computers for everyday tasks. Also, mention that they're error-prone and hard to build because qubits are sensitive to their environment, leading to decoherence.\n",
            "\n",
            "Hmm, but how to explain all this without getting too technical? Use analogies. Like, a qubit is like a light switch that can be on, off, or both at the same time. But maybe a better analogy is a spinning coin vs a coin that's either heads or tails. Superposition allows multiple states at once, so a quantum computer can handle many calculations at the same time.\n",
            "\n",
            "Entanglement could be like two coins that are spinning together; if one lands on heads, the other does too, no matter how far apart. This connection allows qubits to work together\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# Use the async client for concurrent requests\n",
        "async_client = AsyncOpenAI(base_url=BASE_URL, api_key=\"dummy\")\n",
        "\n",
        "async def get_completion(messages):\n",
        "    \"\"\"A helper function to get a single completion asynchronously.\"\"\"\n",
        "    return await async_client.chat.completions.create(\n",
        "        model=SERVED_MODEL_NAME,\n",
        "        messages=messages,\n",
        "        temperature=0.6,\n",
        "        max_tokens=512,\n",
        "    )\n",
        "\n",
        "async def main():\n",
        "    # Create a list of tasks for all our prompts\n",
        "    tasks = [get_completion(msg) for msg in messages_list]\n",
        "\n",
        "    # Run all tasks concurrently and wait for them all to complete\n",
        "    print(\"--- Sending batch requests concurrently ---\")\n",
        "    all_responses = await asyncio.gather(*tasks)\n",
        "    print(\"--- All responses received ---\")\n",
        "\n",
        "    # Extract the content from each response\n",
        "    responses_content = [resp.choices[0].message.content for resp in all_responses]\n",
        "\n",
        "    # Print the results\n",
        "    for i, (p, r) in enumerate(zip(prompts, responses_content), start=1):\n",
        "        print(f\"\\\\nPrompt {i}: {p!r}\")\n",
        "        print(r)\n",
        "\n",
        "# Run the asynchronous main function\n",
        "# In a Jupyter Notebook, you might need to use `await main()` if you are in an async-enabled cell,\n",
        "# or run it like this to handle the event loop.\n",
        "await main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop the server process\n",
        "if 'server_process' in globals() and server_process.poll() is None:\n",
        "    server_process.terminate()\n",
        "    server_process.wait()\n",
        "    print(\"SGLang server stopped.\")\n",
        "else:\n",
        "    print(\"No running server process found to terminate.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nooh9kza7dD9"
      },
      "source": [
        "## Direct Interaction with `curl`\n",
        "\n",
        "For debugging or for use in environments where the OpenAI Python client is not available, you can interact with the SGLang server directly using `curl`.\n",
        "\n",
        "The example below shows how to construct and execute a `curl` command to get a chat completion. We use Python's `subprocess` module to run the command and `json` to parse the output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hR0fMhFz7dD9",
        "outputId": "b3d9d818-d2c6-44b9-c660-2c7f2721f7e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Executing Curl Command ---\n",
            "\n",
            "curl -s http://localhost:33272/v1/chat/completions \\\n",
            "  -H \"Content-Type: application/json\" \\\n",
            "  -d '{\"model\": \"nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\", \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}], \"temperature\": 0.0}'\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\n--- Server Response ---\n",
            "{'id': 'ef64aa03bc164ca2bd95ab73bd117f6e', 'object': 'chat.completion', 'created': 1758151210, 'model': 'nvidia/Llama-3_3-Nemotron-Super-49B-v1_5', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"<think>\\nOkay, so the user is asking for the capital of France. Let me start by recalling what I know about France. France is a country in Europe, known for its rich history, culture, and famous landmarks. The capital is the city where the government is based, right? I think the capital of France is Paris. Wait, but I should make sure I'm not confusing it with other cities. Let me think.\\n\\nI remember that Paris is a major city in France, famous for the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral. But is it definitely the capital? Sometimes countries change their capitals, but I don't think France has done that recently. Let me check some other facts. The president of France, Emmanuel Macron, his official residence is in Paris, I believe. The √âlys√©e Palace is the official residence, which is in Paris. That supports the idea that Paris is the capital.\\n\\nAlso, when I think of other European capitals, like Rome for Italy, Berlin for Germany, Madrid for Spain, and so on. Paris fits into that list. I don't recall any other city in France being the capital. Maybe historically there were different capitals? For example, during different periods in history, like the French Revolution or the different regimes, the capital might have moved. But currently, the capital is Paris. \\n\\nAnother way to verify is to think about international organizations or events. For instance, the headquarters of UNESCO is in Paris, which is a United Nations agency. That might indicate that Paris is a significant city, possibly the capital. Also, major international treaties or events often take place in capitals. The Paris Agreement on climate change was signed there, which again points to Paris being the capital.\\n\\nWait, but could there be any confusion with other cities? For example, Lyon or Marseille are big cities in France, but they aren't the capital. Bordeaux is another major city, but again, not the capital. So I think Paris is the correct answer here. \\n\\nI should also consider if there's any recent change. Sometimes countries change their capitals for various reasons, like administrative or symbolic reasons. But as far as I know, France hasn't changed its capital. The last time I checked, Paris was still the capital. \\n\\nIn summary, based on historical knowledge, current government residence, presence of major institutions, and common knowledge, the capital of France is Paris. I don't see any conflicting information that would suggest otherwise. Therefore, the answer should be Paris.\\n</think>\\n\\nThe capital of France is **Paris**. It is the political, cultural, and economic center of the country, home to iconic landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral. The French government, including the President's official residence (√âlys√©e Palace), is based in Paris, confirming its status as the capital.\", 'reasoning_content': None, 'tool_calls': None}, 'logprobs': None, 'finish_reason': 'stop', 'matched_stop': 128009}], 'usage': {'prompt_tokens': 22, 'total_tokens': 611, 'completion_tokens': 589, 'prompt_tokens_details': None, 'reasoning_tokens': 0}, 'metadata': {'weight_version': 'default'}}\n"
          ]
        }
      ],
      "source": [
        "import subprocess, json\n",
        "\n",
        "# Construct the JSON payload as a Python dictionary first\n",
        "payload = {\n",
        "    \"model\": SERVED_MODEL_NAME,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
        "    ],\n",
        "    \"temperature\": 0.0\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a JSON string\n",
        "payload_str = json.dumps(payload)\n",
        "\n",
        "# Form the curl command\n",
        "# Note: Using f-strings and subprocess like this is convenient for notebooks,\n",
        "# but be cautious about shell injection in production environments.\n",
        "curl_command = f\"\"\"\n",
        "curl -s http://localhost:{port}/v1/chat/completions \\\\\n",
        "  -H \"Content-Type: application/json\" \\\\\n",
        "  -d '{payload_str}'\n",
        "\"\"\"\n",
        "\n",
        "print(\"--- Executing Curl Command ---\")\n",
        "print(curl_command)\n",
        "\n",
        "# Execute the command and load the JSON response\n",
        "response_bytes = subprocess.check_output(curl_command, shell=True)\n",
        "response = json.loads(response_bytes)\n",
        "\n",
        "print(\"\\\\n--- Server Response ---\")\n",
        "print_highlight(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resource Notes\n",
        "\n",
        "- **Hardware**: Nemotron-49B-v1.5 is a large model. Multi-GPU tensor parallel (`--tp`) is highly recommended for acceptable performance.\n",
        "- **Quantization**: For environments with limited resources, consider using quantized versions of the model if available. These can significantly reduce memory usage at the cost of some accuracy.\n",
        "- **Network**: Ensure you have sufficient network and disk bandwidth for the initial model download, as the weights are very large.\n",
        "\n",
        "## Conclusion and Next Steps\n",
        "Congratulations! You successfully deployed the `Nemotron-49B-v1.5` model using SGLang.\n",
        "\n",
        "In this notebook, you have learned how to:\n",
        "- Set up your environment and install SGLang.\n",
        "- Launch and manage an OpenAI-compatible SGLang server.\n",
        "- Perform basic chat, streaming, and batch inference.\n",
        "- Use the model's different reasoning modes.\n",
        "\n",
        "You can adapt tensor parallelism, ports, and sampling parameters to your hardware and application needs.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sgl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
