{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Nemotron-49B-v1.5 with vLLM on NVIDIA GPUs\n",
    "\n",
    "This notebook provides a comprehensive guide on how to run the `Nemotron-49B-v1.5` model using vLLM, a high-performance library for LLM inference and serving.\n",
    "\n",
    "This notebook is divided into two parts:\n",
    "- **Part 1:** Demonstrates how to use the direct vLLM Python API for inference, including batch generation and pseudo-streaming.\n",
    "- **Part 2:** Covers how to deploy the model with an OpenAI-compatible web server for robust chat, streaming, and tool-use capabilities.\n",
    "\n",
    "#### Launch on NVIDIA Brev\n",
    "You can simplify the environment setup by using [NVIDIA Brev](https://developer.nvidia.com/brev). Click the button below to launch this project on a Brev instance with the necessary dependencies pre-configured.\n",
    "\n",
    "Once deployed, click on the \"Open Notebook\" button to get started with this guide.\n",
    "\n",
    "[![Launch on Brev](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-32vt7HcQjCUpafGyquLZwJdIm8F)\n",
    "\n",
    "- Model card: [nvidia/Llama-3.3-Nemotron-Super-49B-v1.5](https://huggingface.co/nvidia/Llama-3.3-Nemotron-Super-49B-v1.5)\n",
    "- vLLM Docs: [https://docs.vllm.ai/](https://docs.vllm.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Part 1: Inference with the Python API](#Part-1:-Inference-with-the-Python-API)\n",
    "  - [Prerequisites](#Prerequisites)\n",
    "  - [Setup](#Setup)\n",
    "  - [Loading the Model](#Loading-the-Model)\n",
    "  - [Single and Batch Generation](#Single-and-Batch-Generation)\n",
    "  - [Streaming (Pseudo)](#Streaming-(Pseudo))\n",
    "- [Part 2: OpenAI-Compatible Server](#Part-2:-OpenAI-Compatible-Server)\n",
    "  - [Launch Server](#Launch-Server)\n",
    "  - [Client Setup](#Client-Setup)\n",
    "  - [Chat and Streaming](#Chat-and-Streaming)\n",
    "  - [Reasoning Modes (`think` vs. `no_think`)](#Reasoning-Modes-(`think`-vs.-`no_think`))\n",
    "  - [Interaction with `curl`](#Interaction-with-`curl`)\n",
    "  - [Cleanup](#Cleanup)\n",
    "- [Resource Notes](#Resource-Notes)\n",
    "- [Conclusion](#Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Inference with the Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "**Hardware:** This notebook requires a machine with at least **2 NVIDIA GPUs** with sufficient VRAM to hold the 49B parameter model.\n",
    "\n",
    "**Software:**\n",
    "- Python 3.10+\n",
    "- CUDA 12.x\n",
    "- PyTorch 2.3+\n",
    "- vLLM 0.10.x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.venv/bin/python3: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install -U \"vllm>=0.10.2,<0.11\" transformers torch \"flashinfer-python>=0.1.6\" openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shadeform/miniconda3/envs/nemotron/lib/python3.13/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Num GPUs: 1\n",
      "GPU[0]: NVIDIA H200\n"
     ]
    }
   ],
   "source": [
    "# GPU environment check\n",
    "import torch\n",
    "import platform\n",
    "\n",
    "print(f\"Python: {platform.python_version()}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Num GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU[{i}]: {props.name} | SM count: {props.multi_processor_count} | Mem: {props.total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "MODEL_ID = \"nvidia/Llama-3_3-Nemotron-Super-49B-v1.5\"\n",
    "\n",
    "llm = LLM(\n",
    "    model=MODEL_ID,\n",
    "    dtype=\"bfloat16\",\n",
    "    trust_remote_code=True,\n",
    "    max_model_len=65536,\n",
    "    gpu_memory_utilization=0.95,\n",
    "    tensor_parallel_size=1,\n",
    ")\n",
    "\n",
    "print(\"Model ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showcasing Reasoning Modes: `think` vs. `no_think`\n",
    "\n",
    "The Nemotron model supports two reasoning modes, which can be controlled via the system message:\n",
    "\n",
    "1.  **Reasoning ON (default):** The model generates a `<think>` block with its reasoning process before the answer.\n",
    "2.  **Reasoning OFF (`/no_think`):** By adding `/no_think` to the system prompt, the model provides a direct answer without the `<think>` block. This is useful for simple tasks where you want a concise response.\n",
    "\n",
    "Since we are using the `vllm` python client which does not use the chat template, we will demonstrate this feature in the OpenAI-compatible server section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single and Batch Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df793b5a507415aba9d36a65f290ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fea74cd98d54ba79f27cb4ae93f9840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It’s a new version of the popular Nemotron font, which was originally designed for the Commodore 64 computer. Version 2.0 of Nemotron Super includes a full set of 96 characters, including uppercase and lowercase letters, numbers, and symbols. The font is designed to look like the text you'd see on a retro computer or video game screen. It’s perfect for creating a nostalgic or vintage look in your designs.\n",
      "\n",
      "Nemotron Super is a monospaced font, meaning that each character takes up the same amount of horizontal space. This makes it ideal for use in programming, coding, or any situation where alignment is important. The font’s design is inspired by the pixelated text of early computing and gaming, giving it a distinctive and charming aesthetic.\n",
      "\n",
      "One of the standout features of Nemotron Super is its versatility. It can be used in a variety of contexts, from web design and digital interfaces to print media and logos. The font’s retro style can add a unique\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20928fe48e4645e08b5163dae155a919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b166d9c8465d49a48ab4857250167138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 1: 'Hello, my name is'\n",
      " (Your Name), and I am a [Your Profession/Student/Parent, etc.] from [Your Location]. I am writing to express my strong support for the proposed regulations to [Briefly Mention the Policy or Regulation, e.g., \"strengthen background checks for firearm purchases\" or \"increase funding for renewable energy projects\"]. \n",
      "\n",
      "I believe that [Policy/Regulation] is a critical step towards [Explain the Main Benefit, e.g., \"ensuring public safety\" or \"addressing climate change\"]. As someone who [Personal Connection, e.g., \"has been affected by gun violence\" or \"is passionate about environmental conservation\"], I have seen firsthand the importance of [Reiterate the Policy's Goal, e.g., \"preventing tragedies\" or \"promoting clean energy\"].\n",
      "\n",
      "The current [Current Situation, e.g., \"lax regulations on gun sales\" or \"reliance on fossil fuels\"] poses significant risks to [Affected Group or Community, e.g.,\n",
      "\n",
      "Prompt 2: 'The capital of France is'\n",
      " Paris, and the capital of Japan is Tokyo. The capital of the United States is Washington, D.C. The capital of Australia is Canberra, and the capital of Canada is Ottawa. The capital of Brazil is Brasília, and the capital of South Africa is Pretoria (administrative), Cape Town (legislative), and Bloemfontein (judicial). The capital of Egypt is Cairo, and the capital of India is New Delhi. The capital of Mexico is Mexico City, and the capital of Russia is Moscow. The capital of China is Beijing, and the capital of Germany is Berlin. The capital of Italy is Rome, and the capital of Spain is Madrid. The capital of Argentina is Buenos Aires, and the capital of South Korea is Seoul. The capital of Saudi Arabia is Riyadh, and the capital of Iran is Tehran. The capital of Turkey is Ankara, and the capital of Indonesia is Jakarta. The capital of Nigeria is Abuja, and the capital of the United Kingdom is\n",
      "\n",
      "Prompt 3: 'Explain quantum computing in simple terms:'\n",
      " how it works, its applications, and why it matters\n",
      "Quantum computing is a revolutionary technology that uses the principles of quantum mechanics to perform calculations and solve problems that are intractable for classical computers. Here's a simple explanation:\n",
      "\n",
      "### How It Works:\n",
      "1. **Qubits (Quantum Bits):** Unlike classical bits that are either 0 or 1, qubits can exist in a state of **superposition**, meaning they can be both 0 and 1 simultaneously. This allows quantum computers to process a vast number of possibilities at once.\n",
      "2. **Entanglement:** Qubits can be **entangled**, so the state of one qubit is directly related to the state of another, no matter the distance between them. This enables instantaneous coordination and enhances processing power.\n",
      "3. **Quantum Gates:** Similar to logic gates in classical computing, quantum gates manipulate qubits. However, quantum gates operate on probabilities and can create complex states through interference, allowing for more sophisticated\n"
     ]
    }
   ],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "params = SamplingParams(temperature=0.6, max_tokens=200)\n",
    "\n",
    "# Single prompt\n",
    "single = llm.generate([\"What is Nemotron Super?\"], sampling_params=params)\n",
    "print(single[0].outputs[0].text)\n",
    "\n",
    "# Batch prompts\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The capital of France is\",\n",
    "    \"Explain quantum computing in simple terms:\"\n",
    "]\n",
    "outputs = llm.generate(prompts, sampling_params=params)\n",
    "for i, out in enumerate(outputs):\n",
    "    print(f\"\\nPrompt {i+1}: {out.prompt!r}\")\n",
    "    print(out.outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming (Pseudo)\n",
    "\n",
    "vLLM’s Python API is designed for high throughput and returns complete `RequestOutput` objects. For true token-by-token streaming, the OpenAI-compatible server (covered in Part 2) is the recommended approach.\n",
    "\n",
    "However, we can simulate streaming by iterating through the characters of the final generated text. This is useful for seeing the output progressively in a notebook environment but does not reflect true streaming inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5572ef405be4bdd84d84dda785675b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9e89d2f426472cbff015d860925b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  Here are some ideas to consider: their speed, power consumption, heat generation, parallel processing capabilities, use in gaming, scientific computing, or machine learning.\n",
      "\n",
      "Silent, swift and hot,\n",
      "GPUs crunch numbers with flair,\n",
      "Lighting up the screen.\n",
      "\n",
      "Another one:\n",
      "\n",
      "Circuits blaze with speed,\n",
      "Billions of cores work as one,\n",
      "Gaming's heart beats fast.\n",
      "\n",
      "And another:\n",
      "\n",
      "Cool\n"
     ]
    }
   ],
   "source": [
    "def stream_like(prompt: str, llm: LLM, sampling_params: SamplingParams) -> None:\n",
    "    outputs = llm.generate([prompt], sampling_params=sampling_params)\n",
    "    text = outputs[0].outputs[0].text\n",
    "    print(\"Response:\", end=\" \")\n",
    "    for ch in text:\n",
    "        print(ch, end=\"\", flush=True)\n",
    "    print()\n",
    "\n",
    "stream_like(\"Write a haiku about GPUs.\", llm, SamplingParams(temperature=0.7, max_tokens=80))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: OpenAI-Compatible Server\n",
    "\n",
    "vLLM offers an OpenAI-compatible server that allows you to use familiar tools like the OpenAI Python client and `curl`. This is the recommended way to use features like chat templates, streaming, and tool calling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Server\n",
    "\n",
    "Run the following command in your terminal to start the server.\n",
    "\n",
    "```bash\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model \"nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\" \\\n",
    "    --dtype bfloat16 \\\n",
    "    --trust-remote-code \\\n",
    "    --served-model-name nemotron \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 5000 \\\n",
    "    --max-model-len 65536 \\\n",
    "    --gpu-memory-utilization 0.95 \\\n",
    "    --tensor-parallel-size 1 \\\n",
    "    --enable-auto-tool-choice \\\n",
    "    --tool-call-parser llama3_json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# This assumes the server is running on localhost:5000\n",
    "client = OpenAI(base_url=\"http://127.0.0.1:5000/v1\", api_key=\"dummy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat and Streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking for three bullet points about vLLM. Let me start by recalling what vLLM is. I know it's a framework for running large language models efficiently. The main points should highlight its key features.\n",
      "\n",
      "First, I remember that vLLM is designed for high throughput and low latency. That's important for applications needing quick responses. So maybe the first bullet can be about efficient inference with techniques like PagedAttention.\n",
      "\n",
      "Second, it's built on a modular architecture. This allows for customization, like integrating different models or backends. That's a good point for developers who need flexibility.\n",
      "\n",
      "\n",
      "<think>\n",
      "Okay, the user wants a short poem about GPUs. Let me start by recalling what GPUs are. They're graphics processing units, right? Used for rendering images, video, and also for parallel computing tasks like machine learning.\n",
      "\n",
      "Hmm, I need to make the poem engaging and not too technical. Maybe focus on their speed, power, and applications. Words like \"silicon heart\" could personify the GPU. Also, terms like \"rendering light\" or \"shadows dance\" might evoke imagery related to graphics.\n",
      "\n",
      "I should structure it in stanzas, maybe four quatrains. Rhyming scheme could\n"
     ]
    }
   ],
   "source": [
    "# Simple chat completion\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"nemotron\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Give me 3 bullet points about vLLM.\"}\n",
    "    ],\n",
    "    temperature=0.6,\n",
    "    max_tokens=256,\n",
    ")\n",
    "print(\"--- Simple Chat Response ---\")\n",
    "print(resp.choices[0].message.content)\n",
    "\n",
    "# Streaming chat completion\n",
    "print(\"\\n--- Streaming Chat Response ---\")\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"nemotron\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a short poem about GPUs.\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=256,\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in stream:\n",
    "    delta = chunk.choices[0].delta\n",
    "    if delta and delta.content:\n",
    "        print(delta.content, end=\"\", flush=True)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoning Modes (`think` vs. `no_think`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoning ON (default)\n",
    "reasoning_prompt = \"I have 5 apples. I eat 2, then my friend gives me 3 more. How many apples do I have now?\"\n",
    "messages_on = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful reasoning assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": reasoning_prompt},\n",
    "]\n",
    "print(\"--- Reasoning ON ---\")\n",
    "response_on = client.chat.completions.create(\n",
    "    model=\"nemotron\",\n",
    "    messages=messages_on,\n",
    "    temperature=0.0,\n",
    "    max_tokens=512,\n",
    ")\n",
    "print(response_on.choices[0].message.content)\n",
    "\n",
    "\n",
    "# Reasoning OFF using /no_think\n",
    "messages_off = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful reasoning assistant.\\n/no_think\"},\n",
    "    {\"role\": \"user\", \"content\": reasoning_prompt},\n",
    "]\n",
    "print(\"\\n--- Reasoning OFF ---\")\n",
    "response_off = client.chat.completions.create(\n",
    "    model=\"nemotron\",\n",
    "    messages=messages_off,\n",
    "    temperature=0.0,\n",
    "    max_tokens=256,\n",
    ")\n",
    "print(response_off.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `curl` Examples\n",
    "\n",
    "You can also interact with the server directly using `curl`.\n",
    "\n",
    "**Chat completion:**\n",
    "```bash\n",
    "curl -sS -X POST http://127.0.0.1:5000/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"nemotron\",\n",
    "    \"messages\": [\n",
    "      {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ],\n",
    "    \"temperature\": 0.0\n",
    "  }'\n",
    "```\n",
    "\n",
    "**Streaming chat completion:**\n",
    "```bash\n",
    "curl -N -sS -X POST http://127.0.0.1:5000/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"nemotron\",\n",
    "    \"messages\": [\n",
    "      {\"role\": \"user\", \"content\": \"Write a short poem about GPUs.\"}\n",
    "    ],\n",
    "    \"stream\": true\n",
    "  }'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "To stop the OpenAI-compatible server, press `CTRL+C` in the terminal where it is running.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resource Notes\n",
    "\n",
    "- **Hardware**: Nemotron-49B-v1.5 is a large model. For optimal performance, running on a multi-GPU setup with high-speed interconnects (like NVLink) is recommended.\n",
    "- **Quantization**: vLLM supports various quantization techniques that can significantly reduce the memory footprint of the model, allowing it to run on smaller GPUs.\n",
    "- **Chat Templates**: When using the OpenAI-compatible server, vLLM automatically applies the correct chat template for the model, which is crucial for getting properly formatted and accurate responses in conversational tasks.\n",
    "- **Tool Calling**: The `--enable-auto-tool-choice` and `--tool-call-parser` flags enable advanced tool-calling capabilities for the model.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, you have learned how to:\n",
    "- Run inference with the Nemotron-49B-v1.5 model using the vLLM Python API.\n",
    "- Deploy the model as an OpenAI-compatible server.\n",
    "- Interact with the server using both a Python client and `curl` for chat, streaming, and reasoning mode demonstrations.\n",
    "- Utilize the model's reasoning modes for different use cases.\n",
    "\n",
    "This notebook provides a solid foundation for building applications with Nemotron and vLLM.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
