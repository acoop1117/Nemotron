{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaHyLm3S-qfI"
   },
   "source": [
    "# Running Nemotron-49B-v1.5 with TensorRT-LLM on NVIDIA GPUs\n",
    "\n",
    "This notebook provides a comprehensive guide on how to run the `Nemotron-49B-v1.5` model using NVIDIA's TensorRT-LLM for high-performance inference.\n",
    "\n",
    "This notebook is divided into two parts:\n",
    "- **Part 1:** Demonstrates how to use the direct Python API for inference, including batch generation.\n",
    "- **Part 2:** Covers how to deploy the model with an OpenAI-compatible web server and interact with it using an OpenAI client.\n",
    "\n",
    "#### Launch on NVIDIA Brev\n",
    "You can simplify the environment setup by using [NVIDIA Brev](https://developer.nvidia.com/brev). Click the button below to launch this project on a Brev instance with the necessary dependencies pre-configured.\n",
    "\n",
    "Once deployed, click on the \"Open Notebook\" button to get started with this guide.\n",
    "\n",
    "[![Launch on Brev](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-32vt7HcQjCUpafGyquLZwJdIm8F)\n",
    "\n",
    "- Model card: [nvidia/Llama-3.3-Nemotron-Super-49B-v1.5](https://huggingface.co/nvidia/Llama-3.3-Nemotron-Super-49B-v1.5)\n",
    "- TensorRT-LLM Docs: [https://nvidia.github.io/TensorRT-LLM/](https://nvidia.github.io/TensorRT-LLM/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Part 1: Inference with the Python API](#Part-1:-Inference-with-the-Python-API)\n",
    "  - [Prerequisites](#Prerequisites)\n",
    "  - [Setup](#Setup)\n",
    "  - [Inference with Python API](#Inference-with-Python-API)\n",
    "  - [Batch Generation](#Batch-Generation)\n",
    "- [Part 2: OpenAI-Compatible Server](#Part-2:-OpenAI-Compatible-Server)\n",
    "  - [Client Setup and Examples](#Client-Setup-and-Examples)\n",
    "  - [Reasoning Modes (`think` vs. `no_think`)](#Reasoning-Modes-(`think`-vs.-`no_think`))\n",
    "  - [Interaction with `curl`](#Interaction-with-`curl`)\n",
    "  - [Cleanup](#Cleanup)\n",
    "- [Resource Notes](#Resource-Notes)\n",
    "- [Conclusion](#Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQ0d5Wjr-qfK"
   },
   "source": [
    "## Part 1: Inference with the Python API\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "**Hardware:** This notebook requires a machine with at least **2 NVIDIA GPUs** with sufficient VRAM to hold the 49B parameter model. Building the TensorRT-LLM engine is memory-intensive.\n",
    "\n",
    "**Software:**\n",
    "- Python 3.10+\n",
    "- NVIDIA GPU with CUDA 12.x\n",
    "- TensorRT-LLM installed from source or via the container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7CutrbU-qfK",
    "outputId": "5acf5f7b-31e0-4e38-e79d-1c66e7c00a0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[2025-09-17 18:08:58] INFO config.py:54: PyTorch version 2.8.0a0+5228986c39.nv25.6 available.\n",
      "[2025-09-17 18:08:58] INFO config.py:66: Polars version 1.25.2 available.\n",
      "/usr/local/lib/python3.12/dist-packages/modelopt/torch/__init__.py:36: UserWarning: transformers version 4.55.0 is incompatible with nvidia-modelopt and may cause issues. Please install recommended version with `pip install nvidia-modelopt[hf]` if working with HF models.\n",
      "  _warnings.warn(\n",
      "2025-09-17 18:09:00,970 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 1.1.0rc4\n",
      "TensorRT-LLM available\n",
      "TensorRT-LLM version: 1.1.0rc4\n",
      "CUDA available: True\n",
      "Num GPUs: 1\n",
      "GPU[0]: NVIDIA H200\n"
     ]
    }
   ],
   "source": [
    "# Environment checks\n",
    "import sys\n",
    "\n",
    "import tensorrt_llm\n",
    "import torch\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"TensorRT-LLM version: {tensorrt_llm.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Num GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU[{i}]: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4JjKIJG-qfL"
   },
   "source": [
    "## Inference with Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1ulDnj1-qfM",
    "outputId": "6e8cd596-5920-4c08-b78c-b07b469be888"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Model: \u001b[1;32m[1/3]\t\u001b[0mDownloading HF model\n",
      "\u001b[38;20mDownloaded model to /root/.cache/huggingface/hub/models--nvidia--Llama-3_3-Nemotron-Super-49B-v1_5/snapshots/2051b8a94fa75052394507a5c81cd7c2b70e9a4b\n",
      "\u001b[0m\u001b[38;20mTime: 0.350s\n",
      "\u001b[0mLoading Model: \u001b[1;32m[2/3]\t\u001b[0mLoading HF model to memory\n",
      "\u001b[38;20mTime: 1.126s\n",
      "\u001b[0mLoading Model: \u001b[1;32m[3/3]\t\u001b[0mBuilding TRT-LLM engine\n",
      "\u001b[38;20mTime: 324.062s\n",
      "\u001b[0m\u001b[1;32mLoading model done.\n",
      "\u001b[0m\u001b[38;20mTotal latency: 325.539s\n",
      "\u001b[0m\u001b[33;20mrank 0 using MpiPoolSession to spawn MPI processes\n",
      "\u001b[0m2025-09-17 17:13:04,901 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.20.0\n",
      "[TensorRT-LLM][INFO] Refreshed the MPI local session\n",
      "[TensorRT-LLM][INFO] Engine version 0.20.0 found in the config file, assuming engine(s) built by new builder API.\n",
      "[TensorRT-LLM][INFO] Refreshed the MPI local session\n",
      "[TensorRT-LLM][INFO] MPI size: 1, MPI local size: 1, rank: 0\n",
      "[TensorRT-LLM][INFO] Rank 0 is using GPU 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 2048\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 2048\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBeamWidth: 1\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxSequenceLen: 131072\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxDraftLen: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: (131072) * 49\n",
      "[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 0\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxNumTokens: 8192\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxInputLen: 8192 = min(maxSequenceLen - 1, maxNumTokens) since context FMHA and usePackedInput are enabled\n",
      "[TensorRT-LLM][INFO] TRTGptModel If model type is encoder, maxInputLen would be reset in trtEncoderModel to maxInputLen: min(maxSequenceLen, maxNumTokens).\n",
      "[TensorRT-LLM][INFO] Capacity Scheduler Policy: GUARANTEED_NO_EVICT\n",
      "[TensorRT-LLM][INFO] Context Chunking Scheduler Policy: None\n",
      "[TensorRT-LLM][INFO] Loaded engine size: 95201 MiB\n",
      "[TensorRT-LLM][INFO] Engine load time 88400 ms\n",
      "[TensorRT-LLM][INFO] Inspecting the engine to identify potential runtime issues...\n",
      "[TensorRT-LLM][INFO] The profiling verbosity of the engine does not allow this analysis to proceed. Re-build the engine with 'detailed' profiling verbosity to get more diagnostics.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 2208.01 MiB for execution context memory.\n",
      "[TensorRT-LLM][INFO] gatherContextLogits: 0\n",
      "[TensorRT-LLM][INFO] gatherGenerationLogits: 0\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 95178 (MiB)\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 5.98 GB GPU memory for runtime buffers.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 8.04 GB GPU memory for decoder.\n",
      "[TensorRT-LLM][INFO] Memory usage when calculating max tokens in paged kv cache: total: 139.72 GiB, available: 29.11 GiB, extraCostMemory: 0.00 GiB\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache primary pool: 4381\n",
      "[TensorRT-LLM][INFO] Number of blocks in KV cache secondary pool: 0, onboard blocks to primary memory before reuse: true\n",
      "[TensorRT-LLM][INFO] before Create KVCacheManager cacheTransPreAllocaSize:0\n",
      "[TensorRT-LLM][INFO] Max KV cache pages per sequence: 4096 [window size=131072]\n",
      "[TensorRT-LLM][INFO] Number of tokens per block: 32.\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 26.20 GiB for max tokens in paged KV cache (140192).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed requests:   0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/tensorrt_llm/llmapi/utils.py:424: UserWarning: LLM API is running in async mode because you have a running event loop, but you are using sync API. This may lead to potential performance loss.\n",
      "  warnings.warn(\n",
      "Processed requests: 100%|██████████| 1/1 [00:11<00:00, 11.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " How does it work?\n",
      "Nemotron Super is a powerful and versatile audio effect plugin designed for music producers, sound designers, and audio engineers. It is a dynamic equalizer that offers a wide range of tonal shaping capabilities, allowing users to enhance or cut specific frequency ranges in their audio signals. The plugin is known for its intuitive interface and advanced features, making it a valuable tool in both mixing and mastering contexts.\n",
      "\n",
      "### Key Features of Nemotron Super:\n",
      "\n",
      "1. **Dynamic Equalization**: Nemotron Super allows for dynamic EQ adjustments, meaning it can automatically adjust the gain of specific frequency bands based on the input signal. This is particularly useful for taming problematic frequencies that only appear intermittently in a track.\n",
      "\n",
      "2. **Multi-band Processing**: The plugin typically supports multiple bands, each of which can be set to boost or cut frequencies within a specific range. This allows for precise control over the tonal balance of an audio signal.\n",
      "\n",
      "3. **Filter Types**: It offers various filter types,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorrt_llm import LLM, SamplingParams\n",
    "\n",
    "MODEL_ID = \"nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\"\n",
    "# Load model\n",
    "llm = LLM(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "# Set sampling parameters\n",
    "params = SamplingParams(temperature=0.6, max_tokens=200)\n",
    "\n",
    "# Generate text\n",
    "result = llm.generate([\"What is Nemotron Super?\"], params)\n",
    "print(result[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ImZsY1Z-qfM"
   },
   "source": [
    "### Batch Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AN8oq52l-qfM",
    "outputId": "436c1b00-5756-497f-d920-bbc05d22873b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed requests: 100%|██████████| 3/3 [00:08<00:00,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 1: 'Hello, my name is'\n",
      " Dr. John Lee and I am a licensed clinical psychologist with over 20 years of experience in the mental health field. I have worked in various settings including hospitals, community mental health centers, and private practice. My areas of expertise include trauma, anxiety, depression, and relationship issues. I am trained in several evidence-based therapies such as Cognitive Behavioral Therapy (CBT), Dialectical Behavior Therapy (DBT), and Eye Movement Desensitization and Reprocessing (EMDR). I believe in a collaborative approach to therapy, working with clients to identify their strengths and develop practical strategies to achieve their goals. I am committed to providing a safe, non-judgmental space for individuals to explore their concerns and work towards healing and growth.\n",
      "I am originally from the Midwest and have lived in several parts of the country, which has given me a broad perspective on different cultures and backgrounds. I value diversity and strive to be culturally sensitive in my work. In my free time, I enjoy hiking,\n",
      "\n",
      "Prompt 2: 'The capital of France is'\n",
      " Paris. It is one of the most visited cities in the world. Paris is known for its art, fashion, and culture. The Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral are some of the famous landmarks in Paris. The city is also known for its romantic atmosphere, which is why it is often called the \"City of Love.\" Paris has a rich history and has been the center of many important events throughout history. The city is home to many universities and research institutions, making it a hub for education and innovation. The cuisine in Paris is also world-renowned, with many Michelin-starred restaurants and charming cafes. Overall, Paris is a vibrant and diverse city that offers something for everyone.\n",
      "Paris is located in the north-central part of France, along the Seine River. It is the capital and most populous city of France. The city has a population of around 2.1 million people within its administrative limits, but the larger metropolitan area has over\n",
      "\n",
      "Prompt 3: 'Explain quantum computing in simple terms:'\n",
      " How does it work?\n",
      "Quantum computing is a type of computing that uses quantum bits or qubits instead of classical bits. Classical bits are either 0 or 1, but qubits can be both 0 and 1 at the same time, thanks to a property called superposition. This allows quantum computers to process a vast number of possibilities simultaneously, making them potentially much faster than classical computers for certain tasks.\n",
      "\n",
      "Here's a simple analogy to understand how quantum computing works:\n",
      "\n",
      "Imagine you have a maze, and you want to find the shortest path from the start to the finish. A classical computer would try each path one by one, like a person walking through the maze. A quantum computer, on the other hand, could explore all paths at the same time, thanks to superposition. This is because qubits can represent multiple states simultaneously, enabling the computer to evaluate many solutions in parallel.\n",
      "\n",
      "Another key concept in quantum computing is entanglement. When qubits are entangled, the state\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Multiple prompts for batch generation\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The capital of France is\",\n",
    "    \"Explain quantum computing in simple terms:\",\n",
    "]\n",
    "\n",
    "results = llm.generate(prompts, params)\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"\\nPrompt {i + 1}: {prompts[i]!r}\")\n",
    "    print(r.outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0wIsHGQ-qfN"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 2: OpenAI-Compatible Server\n",
    "\n",
    "Next, we'll launch a server that's compatible with the OpenAI API. This is a convenient way to integrate the model into existing applications that use the OpenAI client libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v64TnC5m-qfN"
   },
   "source": [
    "Start the OpenAI-compatible server with the following command in your terminal. This will build the TensorRT engine, which may take some time.\n",
    "\n",
    "```bash\n",
    "trtllm-serve \"nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\" \\\n",
    "    --trust_remote_code \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 5000\n",
    "```\n",
    "\n",
    "You should see the following output when the server is ready:\n",
    "```\n",
    "INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qDrqQ8l-qfO"
   },
   "source": [
    "### Client Setup and Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aaomQQeO-qfO",
    "outputId": "b7ea4317-564b-4759-8ff9-0d6e8d0c86a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-17 18:17:46] INFO _client.py:1025: HTTP Request: POST http://127.0.0.1:5000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-09-17 18:17:46] INFO _client.py:1025: HTTP Request: POST http://127.0.0.1:5000/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking for three bullet points about TensorRT-LLM. Let me start by recalling what I know about TensorRT-LLM. It's related to NVIDIA's TensorRT, which is a toolkit for optimizing deep learning models for inference. But TensorRT-LLM specifically targets large language models, right?\n",
      "\n",
      "First, I should explain what TensorRT-LLM is. It's an open-source library, so that's a key point. It's designed to optimize and deploy LLMs efficiently. Maybe mention that it's built on TensorRT, which is known for high-performance inference.\n",
      "\n",
      "Next, the main features. Optimization techniques like model pruning, quantization, and efficient attention mechanisms come to mind. These help reduce the model size and speed up inference without losing much accuracy. Also, integration with frameworks like Hugging Face Transformers would be important for developers.\n",
      "\n",
      "Third, use cases. TensorRT-LLM is used in applications requiring real-time or low-latency responses, such as chatbots, virtual assistants, and content generation. Highlighting deployment on NVIDIA GPUs makes sense since TensorRT is optimized for their hardware.\n",
      "\n",
      "Wait, should I mention specific models it supports? Maybe not necessary for bullet points. Focus on the key aspects:\n",
      "<think>\n",
      "Okay, the user wants a short poem about GPUs. Let me start by recalling what GPUs are. They're Graphics Processing Units, right? Used for rendering images, video, and also for parallel computing tasks. So I need to highlight their power and versatility.\n",
      "\n",
      "Hmm, how to make a poem about that. Maybe start with something about speed or processing. Words like \"silicon heart\" or \"circuits\" could work. I should mention their role in gaming, since that's a common use. Also, maybe touch on their use in AI or machine learning, as that's a big area now.\n",
      "\n",
      "Rhyme scheme? Maybe AABB or ABAB. Let's go with quatrains for simplicity. First line could be about the GPU's core function. \"In circuits deep where data streams collide,\" – that sets a scene. Then talk about rendering worlds or something. \"A titan of speed, the GPU resides,\" – introduces the GPU as a powerful entity.\n",
      "\n",
      "Next stanza: Maybe about gaming. \"It paints the screen with vibrant, fleeting light,\" – rendering graphics. \"A dance of pixels in the dead of night.\" Adds some imagery. Then mention the heat and power, \"Its core ablaze with calculations bright,\" – shows it's working\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from openai import OpenAI\n",
    "\n",
    "BASE_URL = \"http://127.0.0.1:5000/v1\"\n",
    "API_KEY = \"tensorrt_llm\"\n",
    "client = OpenAI(base_url=BASE_URL, api_key=API_KEY)\n",
    "\n",
    "# Get the served model name\n",
    "model_id = requests.get(f\"{BASE_URL}/models\").json()[\"data\"][0][\"id\"]\n",
    "\n",
    "# Basic chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Give me 3 bullet points about TensorRT-LLM.\"}],\n",
    "    temperature=0.6,\n",
    "    max_tokens=256,\n",
    ")\n",
    "print(\"--- Simple Chat Response ---\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Streaming chat completion\n",
    "print(\"\\n--- Streaming Chat Response ---\")\n",
    "stream = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a short poem about GPUs.\"}],\n",
    "    temperature=0.7,\n",
    "    max_tokens=256,\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoning Modes (`think` vs. `no_think`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoning ON (default)\n",
    "reasoning_prompt = (\n",
    "    \"I have 5 apples. I eat 2, then my friend gives me 3 more. How many apples do I have now?\"\n",
    ")\n",
    "messages_on = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful reasoning assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": reasoning_prompt},\n",
    "]\n",
    "print(\"--- Reasoning ON ---\")\n",
    "response_on = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=messages_on,\n",
    "    temperature=0.0,\n",
    "    max_tokens=512,\n",
    ")\n",
    "print(response_on.choices[0].message.content)\n",
    "\n",
    "\n",
    "# Reasoning OFF using /no_think\n",
    "messages_off = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful reasoning assistant.\\n/no_think\"},\n",
    "    {\"role\": \"user\", \"content\": reasoning_prompt},\n",
    "]\n",
    "print(\"\\n--- Reasoning OFF ---\")\n",
    "response_off = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=messages_off,\n",
    "    temperature=0.0,\n",
    "    max_tokens=256,\n",
    ")\n",
    "print(response_off.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QkrvyDH-qfP"
   },
   "source": [
    "### Interaction with `curl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chat Completion:**\n",
    "```bash\n",
    "curl -sS -X POST http://127.0.0.1:5000/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer tensorrt_llm\" \\\n",
    "  -d '{\n",
    "    \"model\": \"nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\",\n",
    "    \"messages\": [\n",
    "      {\"role\": \"user\", \"content\": \"Give me 3 bullet points about TensorRT-LLM.\"}\n",
    "    ],\n",
    "    \"temperature\": 0.6,\n",
    "    \"max_tokens\": 256\n",
    "  }'\n",
    "```\n",
    "\n",
    "**Streaming Chat Completion:**\n",
    "```bash\n",
    "curl -N -sS -X POST http://127.0.0.1:5000/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer tensorrt_llm\" \\\n",
    "  -d '{\n",
    "    \"model\": \"nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\",\n",
    "    \"messages\": [\n",
    "      {\"role\": \"user\", \"content\": \"Write a short poem about GPUs.\"}\n",
    "    ],\n",
    "    \"stream\": true\n",
    "  }'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "To stop the OpenAI-compatible server, press `CTRL+C` in the terminal where it is running.\n",
    "\n",
    "\n",
    "## Resource Notes\n",
    "\n",
    "- **Engine Build Time**: The initial build of the TensorRT-LLM engine can be very time-consuming. This is a one-time cost per model and hardware configuration.\n",
    "- **Hardware**: Nemotron-49B-v1.5 is a large model. Multi-GPU is highly recommended for acceptable performance.\n",
    "- **Quantization**: TensorRT-LLM supports various quantization techniques (like INT8, FP8) to reduce memory usage and improve performance. Refer to the official documentation for more details.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, you have learned how to:\n",
    "- Run inference with the Nemotron-49B-v1.5 model using the TensorRT-LLM Python API.\n",
    "- Perform batch generation for multiple prompts.\n",
    "- Deploy the model as an OpenAI-compatible server.\n",
    "- Interact with the server using both a Python client and `curl`.\n",
    "\n",
    "This provides a solid foundation for integrating high-performance LLM inference into your applications.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
