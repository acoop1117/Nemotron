{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "003f352b",
   "metadata": {},
   "source": [
    "# Tool Calling with Nemotron-Super-49B-v1.5 on NVIDIA NIM\n",
    "\n",
    "This notebook demonstrates how to invoke the `nvidia/llama-3_3-nemotron-super-49b-v1_5` large language model through NVIDIA Inference Microservices (NIM) using the OpenAI-compatible Chat Completions API and walk through a minimal tool-calling loop.\n",
    "\n",
    "- Model card: [NVIDIA Llama-3.3 Nemotron Super 49B v1.5](https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1_5)\n",
    "- NIM Docs: [https://docs.nvidia.com/nim/](https://docs.nvidia.com/nim/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9dad88",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Setup](#Setup)\n",
    "- [API Key Generation](#API-Key-Generation)\n",
    "- [Define Tools](#Define-Tools)\n",
    "- [Run Tool Call](#Run-Tool-Call)\n",
    "- [Troubleshooting](#Troubleshooting)\n",
    "- [Conclusion](#Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e404908",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Python 3.10 or later with `pip`\n",
    "- An NVIDIA API key with access to the NIM endpoints (set `NVIDIA_API_KEY` in your environment)\n",
    "- Network access to reach `https://integrate.api.nvidia.com`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1c3b57",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run the following cell once per environment to install the Python dependencies. If your workspace already has compatible versions, you can skip this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c02b9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade openai python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d827cc3",
   "metadata": {},
   "source": [
    "### API Key Generation\n",
    "Before we get started, generate the API keys to use model from NVIDIA NIM microservice.\n",
    "\n",
    "To generate 'NVIDIA_API_KEY' for NVIDIA NIM microservice:\n",
    "\n",
    "1. Create a free account with [NVIDIA](https://build.nvidia.com/nvidia/nvidia-nemotron-nano-9b-v2).\n",
    "2. Under Input select the Python tab, and click **Get API Key** and then click **Generate Key**.\n",
    "3. Copy and save the generated key as `NVIDIA_API_KEY`. From there, you should have access to the endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e17adb",
   "metadata": {},
   "source": [
    "### Initialize Client\n",
    "\n",
    "The next cell initializes an OpenAI-compatible client configured for the Nemotron NIM endpoint. It reads your NVIDIA API key from the `NVIDIA_API_KEY` environment variable and falls back to a secure prompt if the variable is unset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79ce4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client ready for nvidia/llama-3.3-nemotron-super-49b-v1.5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "NIM_BASE_URL = \"https://integrate.api.nvidia.com/v1\"\n",
    "MODEL_ID = \"nvidia/llama-3.3-nemotron-super-49b-v1.5\"\n",
    "\n",
    "api_key = os.getenv(\"NVIDIA_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"Set NVIDIA_API_KEY in your environment or .env before running this cell.\")\n",
    "\n",
    "client = OpenAI(api_key=api_key, base_url=NIM_BASE_URL)\n",
    "print(f\"Client ready for {MODEL_ID}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac1023d",
   "metadata": {},
   "source": [
    "## Define Tools\n",
    "\n",
    "We'll expose a single local function that returns mock weather data. The model will call this tool when it needs structured information and we'll feed the result back into the conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06f1bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_weather(location: str, unit: str = \"celsius\") -> dict:\n",
    "    \"\"\"Return dummy weather data for the requested location.\"\"\"\n",
    "    forecast = {\n",
    "        \"temperature_c\": 19,\n",
    "        \"temperature_f\": 66,\n",
    "        \"condition\": \"clear skies\",\n",
    "        \"humidity\": 0.72,\n",
    "        \"wind_kph\": 8.0,\n",
    "    }\n",
    "    if unit.lower().startswith(\"f\"):\n",
    "        temperature = forecast[\"temperature_f\"]\n",
    "        unit_label = \"F\"\n",
    "    else:\n",
    "        temperature = forecast[\"temperature_c\"]\n",
    "        unit_label = \"°C\"\n",
    "\n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"summary\": forecast[\"condition\"],\n",
    "        \"temperature\": temperature,\n",
    "        \"unit\": unit_label,\n",
    "        \"humidity\": forecast[\"humidity\"],\n",
    "        \"wind_kph\": forecast[\"wind_kph\"],\n",
    "    }\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Look up the current weather for a city and return structured conditions.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"City and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": \"Temperature unit to return.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6aea31",
   "metadata": {},
   "source": [
    "## Run Tool Call\n",
    "\n",
    "The snippet below sends a chat completion request with the tool definition, inspects the tool call returned by the model, executes the local function, and then submits the tool result back to the model to obtain a final natural-language answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef5f17cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant requested tool:\n",
      "ChatCompletionMessage(content='<think>\\nOkay, the user is asking about the weather in San Francisco this afternoon. Let me check what tools I have available. There\\'s the get_current_weather function, which requires a location and optionally a unit. The user didn\\'t specify the unit, so I\\'ll default to celsius or fahrenheit? The function\\'s parameters have an enum for unit, but since it\\'s not required, maybe I should just proceed without it. Wait, the required field is only location. So I can call get_current_weather with location as \"San Francisco, CA\". The user mentioned \"this afternoon\", but the function is for current weather. Hmm, but maybe the current weather can give an idea of what it\\'s like now, which might be similar to the afternoon. Alternatively, if the function can provide a forecast, but looking at the description, it says \"current weather\". So the function might not provide a forecast. But the user is asking for this afternoon. However, since the available tool is only for current weather, I should use that. Maybe the user is okay with the current conditions as an approximation. So I\\'ll proceed to call get_current_weather with San Francisco, CA. If the user needs a forecast, but the tool doesn\\'t support it, then I might have to inform them, but since the tool is only for current weather, I should use it. So the tool call would be to get_current_weather with location \"San Francisco, CA\". The unit isn\\'t specified, so maybe the function has a default. The parameters have \"unit\" as optional with enum celsius or fahrenheit. Since the user didn\\'t specify, perhaps I should choose one, but the function might handle it. Wait, the strict parameter is set to false, so maybe the function can accept the call without the unit. So the tool call would be {\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"San Francisco, CA\"}}. That should retrieve the current weather data for San Francisco.\\n</think>\\n\\n', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='gRjd3GzCa', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_current_weather'), type='function')], reasoning_content=None)\n",
      "Tool response:\n",
      "{'location': 'San Francisco, CA', 'summary': 'clear skies', 'temperature': 19, 'unit': '°C', 'humidity': 0.72, 'wind_kph': 8.0}\n",
      "\n",
      "Final model answer:\n",
      "\n",
      "<think>\n",
      "Okay, the user asked about the weather in San Francisco this afternoon. I called the get_current_weather tool with the location set to San Francisco, CA. The response came back with clear skies, 19°C, 72% humidity, and 8 km/h wind.\n",
      "\n",
      "Now I need to present this information in a friendly and helpful way. Let me start by stating the current conditions. Mention the clear skies first since that's the summary. Then the temperature, making sure to note the unit is Celsius. Include the humidity and wind speed as well. Maybe add a suggestion about what to wear or if they need an umbrella, but since it's clear skies, probably no rain. Keep it concise but informative. Check if all the data from the tool is included and formatted correctly. Alright, that should cover the user's query.\n",
      "</think>\n",
      "\n",
      "The current weather in San Francisco, CA is **clear skies** with a temperature of **19°C**. Here's the detailed breakdown:  \n",
      "- **Humidity**: 72%  \n",
      "- **Wind**: 8.0 km/h  \n",
      "\n",
      "Perfect weather for a light jacket and outdoor activities! Let me know if you need further details. ☀️\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful weather assistant. Call tools when you need real-world data before replying to the user.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather like in San Francisco this afternoon?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "first_completion = client.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=512,\n",
    ")\n",
    "assistant_message = first_completion.choices[0].message\n",
    "print(\"Assistant requested tool:\")\n",
    "print(assistant_message)\n",
    "\n",
    "assistant_dict = {\n",
    "    \"role\": assistant_message.role,\n",
    "    \"content\": assistant_message.content or \"\",\n",
    "}\n",
    "if assistant_message.tool_calls:\n",
    "    assistant_dict[\"tool_calls\"] = [\n",
    "        {\n",
    "            \"id\": call.id,\n",
    "            \"type\": call.type,\n",
    "            \"function\": {\n",
    "                \"name\": call.function.name,\n",
    "                \"arguments\": call.function.arguments,\n",
    "            },\n",
    "        }\n",
    "        for call in assistant_message.tool_calls\n",
    "    ]\n",
    "\n",
    "messages.append(assistant_dict)\n",
    "\n",
    "for call in assistant_message.tool_calls or []:\n",
    "    if call.function.name != \"get_current_weather\":\n",
    "        continue\n",
    "    call_args = json.loads(call.function.arguments or \"{}\")\n",
    "    tool_response = get_current_weather(**call_args)\n",
    "    print(\"Tool response:\")\n",
    "    print(tool_response)\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": call.id,\n",
    "            \"name\": call.function.name,\n",
    "            \"content\": json.dumps(tool_response),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "final_completion = client.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=messages,\n",
    "    temperature=0.2,\n",
    "    max_tokens=512\n",
    ")\n",
    "final_message = final_completion.choices[0].message\n",
    "print(\"\\nFinal model answer:\\n\")\n",
    "print(final_message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228660b4",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "- Ensure `NVIDIA_API_KEY` is set and has access to the Nemotron NIM deployment; the API returns HTTP 401 if the key is missing or invalid.\n",
    "- When running from a remote environment, verify that outbound HTTPS traffic to `integrate.api.nvidia.com` is allowed.\n",
    "- If the model replies directly without calling a tool, adjust the system prompt or send a follow-up user message requesting structured data to encourage tool use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a53590f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated a simple, complete tool-calling workflow using the `Nemotron-Super-49B-v1.5` model served through NVIDIA NIM. By defining tools and handling the model's tool-use requests, you can build powerful applications that connect LLMs to external data and APIs. From here, you can expand on this example by adding more complex tools, implementing more robust error handling, or integrating real-world APIs.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
