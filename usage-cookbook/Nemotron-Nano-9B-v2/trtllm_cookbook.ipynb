{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running NVIDIA Nemotron Nano 9B v2 with TensorRT-LLM\n",
    "\n",
    "This notebook will walk you through how to run the `nvidia/NVIDIA-Nemotron-Nano-9B-v2` model locally via TensorRT-LLM\n",
    "\n",
    "[TensorRT-LLM](https://nvidia.github.io/TensorRT-LLM/) is NVIDIA’s open-source library for accelerating and optimizing LLM inference performance on NVIDIA GPUs.\n",
    "\n",
    "For more details on the model [click here](https://build.nvidia.com/nvidia/nvidia-nemotron-nano-9b-v2/modelcard)\n",
    "\n",
    "Prerequisites:\n",
    "- NVIDIA GPU with recent drivers (≥ 24 GB VRAM recommended) and CUDA 12.x \n",
    "- Python 3.10+\n",
    "- TensorRT-LLM (you can refer to NVIDIA documentation, or pull this [container](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tensorrt-llm/containers/release?version=1.1.0rc4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites & environment\n",
    "\n",
    "Set up a containerized environment and Jupyter kernel for TensorRT-LLM.\n",
    "\n",
    "If you run the above mentioned container for TRT-LLM, make sure to configure your notebook with it. \n",
    "One approach is:\n",
    "\n",
    "```shell\n",
    "docker run --rm -it --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --gpus=all -p 8888:8888 -p 8000:8000 nvcr.io/nvidia/tensorrt-llm/release:1.1.0rc4\n",
    "\n",
    "pip install jupyter\n",
    "\n",
    "jupyter notebook --ip 0.0.0.0 --no-browser --allow-root\n",
    "```\n",
    "This should output a URL such as: http://127.0.0.1:8888/?token=xxxxxxxxxxxxxxxx\n",
    "\n",
    "Then change your kernel to the Jupyter server running in your Docker container (copy and paste the URL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify GPU\n",
    "\n",
    "Check that CUDA is available and the GPU is detected correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment check\n",
    "import sys\n",
    "\n",
    "import tensorrt_llm\n",
    "import torch\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"TensorRT-LLM version: {tensorrt_llm.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Num GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU[{i}]: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorrt_llm import LLM, SamplingParams\n",
    "from tensorrt_llm.llmapi import KvCacheConfig\n",
    "\n",
    "kv_cache_config = KvCacheConfig(\n",
    "    enable_block_reuse=False,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "llm = LLM(\n",
    "    model=\"nvidia/NVIDIA-Nemotron-Nano-9B-v2\",\n",
    "    max_seq_len=32678,\n",
    "    max_batch_size=4,\n",
    "    kv_cache_config=kv_cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate responses for single or batch prompts\n",
    "\n",
    "Use `SamplingParams` to control generation and run single and batched prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sampling parameters\n",
    "params = SamplingParams(\n",
    "    max_tokens=512,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "# Generate text\n",
    "result = llm.generate([\"Write a haiku about GPUs\"], params)\n",
    "print(result[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple prompts for batch generation\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The capital of France is\",\n",
    "    \"Explain quantum computing in simple terms:\",\n",
    "]\n",
    "\n",
    "results = llm.generate(prompts, params)\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"\\nPrompt {i + 1}: {prompts[i]!r}\")\n",
    "    print(r.outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI-compatible server\n",
    "\n",
    "Start a local OpenAI-compatible server with TensorRT-LLM to serve the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this in a terminal:\n",
    "\n",
    "(Optional) If running via Docker, stop the running Jupyter server, and configure to another Python 3.10+ kernel for simplicity. Start this trtllm server instead. \n",
    "\n",
    "```shell\n",
    "trtllm-serve \"nvidia/NVIDIA-Nemotron-Nano-9B-v2\" \\\n",
    "  --host 0.0.0.0 --port 8000 \\\n",
    "  --max_seq_len 32678 \\\n",
    "  --max_batch_size 4 \\\n",
    "  --extra_llm_api_options <(echo '{\"kv_cache_config\":{\"enable_block_reuse\":false}}')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your server is now running! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the API\n",
    "\n",
    "Use the OpenAI-compatible client to send requests to the local TensorRT-LLM server. \n",
    "\n",
    "Note: The model supports two modes - Reasoning ON (default) vs OFF. These can be toggled by passing /think vs /no_think as a part of the \"system\" message. \n",
    "\n",
    "The /think or /no_think keywords can also be provided in “user” messages for turn-level reasoning control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from openai import OpenAI\n",
    "\n",
    "# Setup client\n",
    "BASE_URL = \"http://127.0.0.1:8000/v1\"\n",
    "API_KEY = \"tensorrt_llm\"\n",
    "client = OpenAI(base_url=BASE_URL, api_key=API_KEY)\n",
    "\n",
    "# Get model ID\n",
    "model_id = requests.get(f\"{BASE_URL}/models\", timeout=10).json()[\"data\"][0][\"id\"]\n",
    "\n",
    "# Basic chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"/no_think\"},\n",
    "        {\"role\": \"user\", \"content\": \"Give me 3 bullet points about TensorRT-LLM.\"},\n",
    "    ],\n",
    "    temperature=0.6,\n",
    "    max_tokens=256,\n",
    ")\n",
    "print(\"Response:\", response.choices[0].message.content)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "# Streaming chat completion\n",
    "print(\"Streaming response:\")\n",
    "stream = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"/think\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a short haiku about GPUs.\"},\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=256,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool calling\n",
    "\n",
    "Use the OpenAI tools schema to call functions via the TensorRT-LLM endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool calling via OpenAI tools schema\n",
    "TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate_tip\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"bill_total\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The total amount of the bill\",\n",
    "                    },\n",
    "                    \"tip_percentage\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The percentage of tip to be applied\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"bill_total\", \"tip_percentage\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"nvidia/NVIDIA-Nemotron-Nano-9B-v2\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\"role\": \"user\", \"content\": \"My bill is $50. What will be the amount for 15% tip?\"},\n",
    "    ],\n",
    "    tools=TOOLS,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    max_tokens=32768,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "print(completion.choices[0].message.tool_calls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemotron-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
