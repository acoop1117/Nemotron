{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tool Calling with Nemotron-Nano-9B-v2 on NVIDIA NIM\n",
        "\n",
        "This notebook demonstrates how to invoke the `nvidia/nemotron-nano-9b-v2` large language model through NVIDIA Inference Microservices (NIM) using the OpenAI-compatible Chat Completions API and walk through a minimal tool-calling loop.\n",
        "\n",
        "- Model card: [NVIDIA Nemotron Nano 9B v2](https://build.nvidia.com/nvidia/nvidia-nemotron-nano-9b-v2)\n",
        "- NIM Docs: [https://docs.nvidia.com/nim/](https://docs.nvidia.com/nim/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "- [Prerequisites](#Prerequisites)\n",
        "- [Setup](#Setup)\n",
        "- [API Key Generation](#API-Key-Generation)\n",
        "- [Define Tools](#Define-Tools)\n",
        "- [Run Tool Call](#Run-Tool-Call)\n",
        "- [Troubleshooting](#Troubleshooting)\n",
        "- [Conclusion](#Conclusion)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "- Python 3.10 or later with `pip`\n",
        "- An NVIDIA API key with access to the NIM endpoints (set `NVIDIA_API_KEY` in your environment)\n",
        "- Network access to reach `https://integrate.api.nvidia.com`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Run the following cell once per environment to install the Python dependencies. If your workspace already has compatible versions, you can skip this step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade openai python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### API Key Generation\n",
        "Before we get started, generate the API keys to use model from NVIDIA NIM microservice.\n",
        "\n",
        "To generate 'NVIDIA_API_KEY' for NVIDIA NIM microservice:\n",
        "\n",
        "1. Create a free account with [NVIDIA](https://build.nvidia.com/nvidia/nvidia-nemotron-nano-9b-v2).\n",
        "2. Under Input select the Python tab, and click **Get API Key** and then click **Generate Key**.\n",
        "3. Copy and save the generated key as `NVIDIA_API_KEY`. From there, you should have access to the endpoints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize Client\n",
        "\n",
        "The next cell initializes an OpenAI-compatible client configured for the Nemotron NIM endpoint. It reads your NVIDIA API key from the `NVIDIA_API_KEY` environment variable or a local `.env` file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Client ready for nvidia/nvidia-nemotron-nano-9b-v2\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "NIM_BASE_URL = \"https://integrate.api.nvidia.com/v1\"\n",
        "MODEL_ID = \"nvidia/nvidia-nemotron-nano-9b-v2\" # <-- Make sure this is the correct model ID for the API\n",
        "\n",
        "api_key = os.getenv(\"NVIDIA_API_KEY\")\n",
        "if not api_key:\n",
        "    raise RuntimeError(\"Set NVIDIA_API_KEY in your environment or .env before running this cell.\")\n",
        "\n",
        "client = OpenAI(api_key=api_key, base_url=NIM_BASE_URL)\n",
        "print(f\"Client ready for {MODEL_ID}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Tools\n",
        "\n",
        "We'll expose a single local function that returns mock weather data. The model will call this tool when it needs structured information and we'll feed the result back into the conversation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_current_weather(location: str, unit: str = \"celsius\") -> dict:\n",
        "    \"\"\"Return dummy weather data for the requested location.\"\"\"\n",
        "    forecast = {\n",
        "        \"temperature_c\": 19,\n",
        "        \"temperature_f\": 66,\n",
        "        \"condition\": \"clear skies\",\n",
        "        \"humidity\": 0.72,\n",
        "        \"wind_kph\": 8.0,\n",
        "    }\n",
        "    if unit.lower().startswith(\"f\"):\n",
        "        temperature = forecast[\"temperature_f\"]\n",
        "        unit_label = \"F\"\n",
        "    else:\n",
        "        temperature = forecast[\"temperature_c\"]\n",
        "        unit_label = \"°C\"\n",
        "\n",
        "    return {\n",
        "        \"location\": location,\n",
        "        \"summary\": forecast[\"condition\"],\n",
        "        \"temperature\": temperature,\n",
        "        \"unit\": unit_label,\n",
        "        \"humidity\": forecast[\"humidity\"],\n",
        "        \"wind_kph\": forecast[\"wind_kph\"],\n",
        "    }\n",
        "\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_current_weather\",\n",
        "            \"description\": \"Look up the current weather for a city and return structured conditions.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"City and state, e.g. San Francisco, CA\",\n",
        "                    },\n",
        "                    \"unit\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
        "                        \"description\": \"Temperature unit to return.\",\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"location\"],\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Tool Call\n",
        "\n",
        "The snippet below sends a chat completion request with the tool definition, inspects the tool call returned by the model, executes the local function, and then submits the tool result back to the model to obtain a final natural-language answer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant requested tool:\n",
            "ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='ndxqwsLFA', function=Function(arguments='{\"location\": \"San Francisco, CA\", \"unit\": \"fahrenheit\"}', name='get_current_weather'), type='function')], reasoning_content='Okay, the user is asking about the weather in San Francisco this afternoon. Let me see. I need to check the current weather for San Francisco. The tools available include get_current_weather, which requires the location and optionally the unit. The user didn\\'t specify Celsius or Fahrenheit, so maybe I should default to one. Since the user is in the US, maybe Fahrenheit is more common there. But the function allows either. Wait, the function\\'s parameters have an enum for unit, so I should choose one. Let me go with Fahrenheit unless told otherwise. The location is clearly San Francisco, CA. So I should call get_current_weather with location \"San Francisco, CA\" and unit \"fahrenheit\". That should give the current conditions. The user mentioned \"this afternoon,\" but the function gives current weather. Maybe the current weather is the best available data. I\\'ll proceed with that.')\n",
            "Tool response:\n",
            "{'location': 'San Francisco, CA', 'summary': 'clear skies', 'temperature': 66, 'unit': 'F', 'humidity': 0.72, 'wind_kph': 8.0}\n",
            "\n",
            "Final model answer:\n",
            "\n",
            "The current weather in San Francisco this afternoon is clear skies with a temperature of 66°F. The humidity is at 72%, and there's a light breeze of 8.0 km/h. You might want to bring a light jacket as it’s a bit cool!\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful weather assistant. Call tools when you need real-world data before replying to the user.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What's the weather like in San Francisco this afternoon?\",\n",
        "    },\n",
        "]\n",
        "\n",
        "first_completion = client.chat.completions.create(\n",
        "    model=MODEL_ID,\n",
        "    messages=messages,\n",
        "    tools=tools,\n",
        "    tool_choice=\"auto\",\n",
        "    temperature=0.2,\n",
        "    max_tokens=512,\n",
        ")\n",
        "assistant_message = first_completion.choices[0].message\n",
        "print(\"Assistant requested tool:\")\n",
        "print(assistant_message)\n",
        "\n",
        "assistant_dict = {\n",
        "    \"role\": assistant_message.role,\n",
        "    \"content\": assistant_message.content or \"\",\n",
        "}\n",
        "if assistant_message.tool_calls:\n",
        "    assistant_dict[\"tool_calls\"] = [\n",
        "        {\n",
        "            \"id\": call.id,\n",
        "            \"type\": call.type,\n",
        "            \"function\": {\n",
        "                \"name\": call.function.name,\n",
        "                \"arguments\": call.function.arguments,\n",
        "            },\n",
        "        }\n",
        "        for call in assistant_message.tool_calls\n",
        "    ]\n",
        "\n",
        "messages.append(assistant_dict)\n",
        "\n",
        "for call in assistant_message.tool_calls or []:\n",
        "    if call.function.name != \"get_current_weather\":\n",
        "        continue\n",
        "    call_args = json.loads(call.function.arguments or \"{}\")\n",
        "    tool_response = get_current_weather(**call_args)\n",
        "    print(\"Tool response:\")\n",
        "    print(tool_response)\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"tool\",\n",
        "            \"tool_call_id\": call.id,\n",
        "            \"name\": call.function.name,\n",
        "            \"content\": json.dumps(tool_response),\n",
        "        }\n",
        "    )\n",
        "\n",
        "final_completion = client.chat.completions.create(\n",
        "    model=MODEL_ID,\n",
        "    messages=messages,\n",
        "    temperature=0.2,\n",
        "    max_tokens=512,\n",
        ")\n",
        "final_message = final_completion.choices[0].message\n",
        "print(\"\\nFinal model answer:\\n\")\n",
        "print(final_message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Troubleshooting\n",
        "\n",
        "- If you get a `404 Not Found` error, the `MODEL_ID` is likely incorrect. Check the NVIDIA build catalog for the exact model identifier for this API.\n",
        "- Ensure `NVIDIA_API_KEY` is set and has access to the Nemotron NIM deployment; the API returns HTTP 401 if the key is missing or invalid.\n",
        "- When running from a remote environment, verify that outbound HTTPS traffic to `integrate.api.nvidia.com` is allowed.\n",
        "- If the model replies directly without calling a tool, adjust the system prompt or send a follow-up user message requesting structured data to encourage tool use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrated a simple, complete tool-calling workflow using the `Nemotron-Nano-9B-v2` model served through NVIDIA NIM. By defining tools and handling the model's tool-use requests, you can build powerful applications that connect LLMs to external data and APIs. From here, you can expand on this example by adding more complex tools, implementing more robust error handling, or integrating real-world APIs.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "data_analysis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
